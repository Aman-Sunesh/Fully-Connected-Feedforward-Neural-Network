{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42bf45a2-09de-4d32-8526-1d799ab77e08",
   "metadata": {},
   "source": [
    "# Mini Project 5 - Classification with Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439276dc-339f-4219-8d99-db62c58b767b",
   "metadata": {},
   "source": [
    "## Instructions on How to Run\n",
    "\n",
    "1. Open the Jupyter Notebook.  \n",
    "2. Ensure you have NumPy, Pandas, Matplotlib, scikit-learn installed.  \n",
    "3. Run each cell in order from top to bottom.  \n",
    "4. The notebook automatically loads the MNIST dataset via `load_digits()`.  \n",
    "5. Each section prints outputs or plots relevant to the project requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf55269-cff1-47cd-a5cc-4a6681dc8ac6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we implement a neural network to classify images of handwritten digits using the MNIST dataset. The network structure is fixed as **[64, 30, 10]** – that is, 64 input neurons (each 8×8 image flattened to a 64-dimensional vector), 30 neurons in the hidden layer, and 10 output neurons corresponding to the digit classes 0 through 9.\n",
    "\n",
    "The neural network is built using a framework that includes:\n",
    "- Forward propagation for computing the network output.\n",
    "- Backward propagation for computing gradients.\n",
    "- Parameter updates through gradient descent.\n",
    "- Evaluation of model performance via prediction and accuracy functions.\n",
    "\n",
    "**Key Idea:**  \n",
    "A non-linear activation function is crucial to allow the network to model complex relationships. In this project, we experiment with **Sigmoid**, **ReLU**, and **Tanh** activations.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Formulas\n",
    "\n",
    "### 1. Activation Functions\n",
    "\n",
    "**Sigmoid Function:**\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Derivative of Sigmoid:**\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)\\,(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "**ReLU Function:**\n",
    "\n",
    "$$\n",
    "\\operatorname{ReLU}(x) = \\max(0,\\, x)\n",
    "$$\n",
    "\n",
    "**Derivative of ReLU:**\n",
    "\n",
    "$$\n",
    "\\operatorname{ReLU}'(x) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } x \\ge 0 \\\\\n",
    "0, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Tanh Function:**\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "**Derivative of Tanh:**\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\tanh(x) = 1 - \\tanh(x)^2\n",
    "$$\n",
    "\n",
    "### 2. Forward Propagation\n",
    "\n",
    "For an input \\( X \\):\n",
    "\n",
    "1. **Hidden Layer:**\n",
    "\n",
    "$$\n",
    "Z^{[1]} = XW^{[1]} + b^{[1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[1]} = f(Z^{[1]})\n",
    "$$\n",
    "\n",
    "2. **Output Layer:**\n",
    "\n",
    "$$\n",
    "Z^{[2]} = A^{[1]}W^{[2]} + b^{[2]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[2]} = f(Z^{[2]})\n",
    "$$\n",
    "\n",
    "Here, f(_) represents any of the activation functions (sigmoid, ReLU, or tanh).\n",
    "\n",
    "### 3. Cost Function (Mean Squared Error)\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{m}\\sum_{i=1}^{m} \\bigl( A^{[2]}_i - y_i \\bigr)^2\n",
    "$$\n",
    "\n",
    "### 4. Backward Propagation\n",
    "\n",
    "**For the Output Layer:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial Z^{[2]}} = 2\\,(A^{[2]} - y) \\odot f'\\bigl(Z^{[2]}\\bigr)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{[2]}} = \\frac{1}{m}\\,(A^{[1]})^T \\frac{\\partial J}{\\partial Z^{[2]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{[2]}} = \\frac{1}{m}\\sum_{i=1}^{m} \\frac{\\partial J}{\\partial Z_i^{[2]}}\n",
    "$$\n",
    "\n",
    "**For the Hidden Layer:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial Z^{[1]}} = \\left( \\frac{\\partial J}{\\partial Z^{[2]}}W^{[2]T} \\right) \\odot f'\\bigl(Z^{[1]}\\bigr)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W^{[1]}} = \\frac{1}{m}\\,X^T \\frac{\\partial J}{\\partial Z^{[1]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b^{[1]}} = \\frac{1}{m}\\sum_{i=1}^{m} \\frac{\\partial J}{\\partial Z_i^{[1]}}\n",
    "$$\n",
    "\n",
    "Here, $$\\odot$$ , denotes element-wise multiplication.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b991e-2904-44f7-b2d7-4687b910f50d",
   "metadata": {},
   "source": [
    "## Importing the Necessary Libraries and the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72eb0f34-8b55-4cd7-baef-d8f3b7514b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits #import the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d3fe9-b371-4377-ba52-7af9d15af9dc",
   "metadata": {},
   "source": [
    "## Data Loading & Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30e33b1-a6b2-4585-a127-67f2b27ea52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122f7343-39af-47dc-9345-339bd415062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show() # The printed figure shows the first 8x8 digit image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6008bc-aa01-4546-b53d-e8aa544538e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images: (1797, 8, 8)\n",
      "Shape of data: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of images and data\n",
    "print(\"Shape of images:\", digits.images.shape)  \n",
    "print(\"Shape of data:\", digits.data.shape)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b26a4-f812-4ace-af2f-96586e4b24a9",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "**Scaling the Inputs:**  \n",
    "- Normalize the 64-dimensional input vectors.\n",
    "\n",
    "**One-Hot Encoding the Targets:**  \n",
    "- Create a one-hot encoded matrix for the 10 digit classes.\n",
    "\n",
    "**Train-Test Split:**  \n",
    "- Use an 80% / 20% train-test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bee2ab0-3279-48fa-a3d2-52b469c14c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d850667e-2058-47b8-92a6-cc6c1db3a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the flattened data as inputs and the original labels as targets\n",
    "inputs = digits.data\n",
    "targets = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55024985-ffff-4476-9b2a-4a8f3f499a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input features\n",
    "inputs = scaler.fit_transform(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e432897-025c-4780-8727-c1ac5cf8639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the targets by creating a new array\n",
    "onehot_targets = np.zeros((targets.shape[0], 10))\n",
    "\n",
    "for i in range(targets.shape[0]):\n",
    "    onehot_targets[i, targets[i]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b397fb-1392-4fd1-84fb-91c0f691e2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef9c780-da71-4091-bbe4-abfd5b815f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% train, 20% test).\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, onehot_targets, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdaa9f-5695-46bb-872a-72c6afa6e77f",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b734a7-8d37-410d-a7f5-441c0db3e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size = 64, hidden_size = 30, output_size = 10, learning_rate = 0.01, epochs = 1000):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Weights & bias initialization with small random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01  \n",
    "        self.b1 = np.zeros((1, hidden_size))                       \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01 \n",
    "        self.b2 = np.zeros((1, output_size))                       #\n",
    "\n",
    "    # Sigmoid activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Derivative of the sigmoid function\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    # ReLU activation function\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # Derivative of the ReLU function\n",
    "    def ReLU_deriv(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    # Tanh activation function\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    # Derivative of the tanh function\n",
    "    def tanh_deriv(self, x):\n",
    "        return 1 - (self.tanh(x))**2\n",
    "\n",
    "    # Forward propagation: computes the activations for the hidden and output layers\n",
    "    def forward_propagation(self, x, activation):\n",
    "        # Hidden layer\n",
    "        z1 = x @ self.W1 + self.b1\n",
    "        a1 = activation(z1)\n",
    "    \n",
    "        # Output layer\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        a2 = activation(z2)\n",
    "    \n",
    "        return z1, a1, z2, a2\n",
    "\n",
    "    # Compute the cost using Mean Squared Error\n",
    "    def compute_cost(self, y_pred, y_true):\n",
    "        cost = np.mean((y_pred - y_true) ** 2)\n",
    "        return cost\n",
    "\n",
    "    # Back propagation: computes gradients for updating weights and biases\n",
    "    def back_propagation(self, x, y, activation, activation_derivative):       \n",
    "        m = y.shape[0]\n",
    "        \n",
    "        z1, a1, z2, a2 = self.forward_propagation(x, activation)\n",
    "\n",
    "        # Compute error at output layer\n",
    "        error2 = 2 * (a2 - y) * activation_derivative(z2)\n",
    "\n",
    "        # Compute gradients for weights and biases for the output layer\n",
    "        grad_W2 = (a1.T @ error2) / m\n",
    "        grad_b2 = np.sum(error2, axis = 0, keepdims = True) / m\n",
    "\n",
    "        # Backpropagate the error to the hidden layer\n",
    "        error1 = (error2 @ self.W2.T) * activation_derivative(z1)\n",
    "\n",
    "        # Compute gradients for weights and biases for the hidden layer\n",
    "        grad_W1 = (x.T @ error1) / m\n",
    "        grad_b1 = np.sum(error1, axis = 0, keepdims = True) / m\n",
    "\n",
    "        return grad_W1, grad_b1, grad_W2, grad_b2\n",
    "\n",
    "    # Update the network parameters (weights and biases) using the computed gradients\n",
    "    def update_parameters(self, grad_W1, grad_b1, grad_W2, grad_b2):\n",
    "        self.W2 = self.W2 - (self.learning_rate * grad_W2)\n",
    "        self.b2 = self.b2 - (self.learning_rate * grad_b2)\n",
    "        self.W1 = self.W1 - (self.learning_rate * grad_W1)\n",
    "        self.b1 = self.b1 - (self.learning_rate * grad_b1)\n",
    "\n",
    "    # Train the neural network by iterating over epochs\n",
    "    def train(self, x, y, activation, activation_derivative):\n",
    "        for epoch in range(self.epochs):\n",
    "            # Perform forward propagation to get outputs\n",
    "            z1, a1, z2, a2 = self.forward_propagation(x, activation)\n",
    "\n",
    "            # Compute gradients via back propagation\n",
    "            grad_W1, grad_b1, grad_W2, grad_b2 = self.back_propagation(x, y, activation, activation_derivative)\n",
    "\n",
    "            # Update network parameters with the gradients\n",
    "            self.update_parameters(grad_W1, grad_b1, grad_W2, grad_b2)\n",
    "\n",
    "            # Every 100 epochs, compute and print the cost for monitoring convergence\n",
    "            if (epoch%100 == 0):\n",
    "                cost = self.compute_cost(a2, y)\n",
    "                print(f\"Epoch {epoch}, Cost: {cost}\")\n",
    "\n",
    "    # Predict class labels for input data\n",
    "    def predict(self, x, activation):\n",
    "        _ , _ , _ , a2 = self.forward_propagation(x, activation)\n",
    "        predictions = np.argmax(a2, axis = 1)  # Choose the index with the highest activation\n",
    "        return predictions\n",
    "\n",
    "    # Compute the accuracy score given the true and predicted labels\n",
    "    def accuracy_score(self, y_true, y_pred):\n",
    "        return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f872f9b-ab03-422c-b57f-3d690737281a",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd096c8a-b7de-47f3-a769-7fd19ce7630f",
   "metadata": {},
   "source": [
    "### Neural Network with Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c51d5d-5f25-4afe-97fd-52b51d854ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 0.2499505353562436\n",
      "Epoch 100, Cost: 0.11768423959628062\n",
      "Epoch 200, Cost: 0.09946498383544228\n",
      "Epoch 300, Cost: 0.09436171368203497\n",
      "Epoch 400, Cost: 0.09232483984670133\n",
      "Epoch 500, Cost: 0.09134410261961073\n",
      "Epoch 600, Cost: 0.090814695476524\n",
      "Epoch 700, Cost: 0.09050613875663747\n",
      "Epoch 800, Cost: 0.09031607767844214\n",
      "Epoch 900, Cost: 0.09019393381835755\n",
      "Sigmoid Test Accuracy: 0.07777777777777778\n"
     ]
    }
   ],
   "source": [
    "# Neural Network with Sigmoid activation\n",
    "\n",
    "net_sigmoid = NeuralNetwork(learning_rate = 0.01, epochs = 1000)\n",
    "net_sigmoid.train(X_train, y_train, activation = net_sigmoid.sigmoid, activation_derivative = net_sigmoid.sigmoid_deriv)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded test labels to class indices\n",
    "predictions = net_sigmoid.predict(X_test, activation = net_sigmoid.sigmoid)\n",
    "\n",
    "acc_sigmoid = net_sigmoid.accuracy_score(y_test_labels, predictions)\n",
    "print(\"Sigmoid Test Accuracy:\", acc_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a70da-8f27-43fa-aa02-520b86cef122",
   "metadata": {},
   "source": [
    "### Neural Network with ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "280226c4-d366-4de1-b309-3ee9af8bc694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 0.09996909688755827\n",
      "Epoch 100, Cost: 0.08889566075149792\n",
      "Epoch 200, Cost: 0.08323848804252669\n",
      "Epoch 300, Cost: 0.06889390613964584\n",
      "Epoch 400, Cost: 0.050605501658236025\n",
      "Epoch 500, Cost: 0.0371807089738276\n",
      "Epoch 600, Cost: 0.029443322067238143\n",
      "Epoch 700, Cost: 0.024662869884151402\n",
      "Epoch 800, Cost: 0.02119527648581149\n",
      "Epoch 900, Cost: 0.018593993844726348\n",
      "ReLU Test Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Neural Network with ReLU activation\n",
    "\n",
    "net_ReLU = NeuralNetwork(learning_rate = 0.01, epochs = 1000)\n",
    "net_ReLU.train(X_train, y_train, activation = net_ReLU.ReLU, activation_derivative = net_ReLU.ReLU_deriv)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded test labels to class indices\n",
    "predictions = net_ReLU.predict(X_test, activation = net_ReLU.ReLU)\n",
    "\n",
    "acc_ReLU = net_ReLU.accuracy_score(y_test_labels, predictions)\n",
    "print(\"ReLU Test Accuracy:\", acc_ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7e583-cdf3-494f-a132-703cc854477c",
   "metadata": {},
   "source": [
    "### Neural Network with Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2ad47ee-613c-4877-82b1-494326a4631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost: 0.09997669382146918\n",
      "Epoch 100, Cost: 0.08241535412456817\n",
      "Epoch 200, Cost: 0.06724263832459602\n",
      "Epoch 300, Cost: 0.057985381839983335\n",
      "Epoch 400, Cost: 0.050486091075251184\n",
      "Epoch 500, Cost: 0.04538922197885836\n",
      "Epoch 600, Cost: 0.04231161263450493\n",
      "Epoch 700, Cost: 0.04030793547949076\n",
      "Epoch 800, Cost: 0.03893110865226537\n",
      "Epoch 900, Cost: 0.03796323174950409\n",
      "tanh Test Accuracy: 0.9388888888888889\n"
     ]
    }
   ],
   "source": [
    "# Neural Network with tanh activation\n",
    "\n",
    "net_tanh = NeuralNetwork(learning_rate = 0.01, epochs = 1000)\n",
    "net_tanh.train(X_train, y_train, activation = net_tanh.tanh, activation_derivative = net_tanh.tanh_deriv)\n",
    "\n",
    "y_test_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoded test labels to class indices\n",
    "predictions = net_tanh.predict(X_test, activation = net_tanh.tanh)\n",
    "\n",
    "acc_tanh = net_tanh.accuracy_score(y_test_labels, predictions)\n",
    "print(\"tanh Test Accuracy:\", acc_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958e612-7753-44d2-947b-8e56bd4b71e7",
   "metadata": {},
   "source": [
    "## Observations & Findings\n",
    "\n",
    "**(i) Sigmoid Activation:**\n",
    "- The cost decreases from approximately 0.25 to 0.09, which indicates that the network is fitting the training data. However, the test accuracy remains very low at around 7.78%. It appears that the network might be predicting nearly the same class for all test examples, which is a common issue when using sigmoid activation in certain network setups due to vanishing gradients. Although the code is correct, the chosen parameters or the sigmoid activation function may not be well-suited for the architecture and dataset under the current hyperparameter settings. In the next section, we will examine whether adjusting the hyperparameters can improve the accuracy with sigmoid activation.\n",
    "\n",
    "**(ii) ReLU Activation:**\n",
    "- The cost steadily decreases from roughly 0.0999 to 0.0186, which is a positive sign of convergence. Moreover, the test accuracy is approximately 93.33%, indicating good performance.\n",
    "\n",
    "**(iii) tanh Activation:**\n",
    "- The cost decreases from around 0.0999 to 0.0379, showing smooth convergence. Additionally, the test accuracy is about 93.88%, which is very promising. It is possible that the accuracy for both ReLU and tanh could be further improved by fine-tuning the hyperparameters, a subject we will explore in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aca8eb-819c-4aff-b6b0-5c2d220d55af",
   "metadata": {},
   "source": [
    "## Experimenting with Different Hyperparameters\n",
    "\n",
    "We then perform a grid search over the following hyperparameters:\n",
    "\n",
    "- **Learning Rates:** `[0.001, 0.005, 0.01, 0.05, 0.1]`\n",
    "- **Epoch Values:** `[500, 1000, 1500, 2000, 2500]`\n",
    "- **Activation Functions:** `Sigmoid`, `ReLU`, `Tanh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e5e401-8a14-494f-bb0e-80835051dd4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experimenting with sigmoid activation ===\n",
      "\n",
      "Epoch 0, Cost: 0.25072959556321894\n",
      "Epoch 100, Cost: 0.22005333431261223\n",
      "Epoch 200, Cost: 0.19572630360967855\n",
      "Epoch 300, Cost: 0.17662598925050382\n",
      "Epoch 400, Cost: 0.1616505280071743\n",
      "LR: 0.001  Epochs: 500   -> Accuracy: 0.0833\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.24801756242615614\n",
      "Epoch 100, Cost: 0.21789463109964927\n",
      "Epoch 200, Cost: 0.19403075441934362\n",
      "Epoch 300, Cost: 0.17529940429468083\n",
      "Epoch 400, Cost: 0.1606101827634324\n",
      "Epoch 500, Cost: 0.14903925745661736\n",
      "Epoch 600, Cost: 0.13985490858447958\n",
      "Epoch 700, Cost: 0.13249709451497654\n",
      "Epoch 800, Cost: 0.12654384951837247\n",
      "Epoch 900, Cost: 0.12167878265734537\n",
      "LR: 0.001  Epochs: 1000  -> Accuracy: 0.0944\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.25463537371939854\n",
      "Epoch 100, Cost: 0.22318550119697184\n",
      "Epoch 200, Cost: 0.19819797169887762\n",
      "Epoch 300, Cost: 0.17856567556161732\n",
      "Epoch 400, Cost: 0.16317534285283658\n",
      "Epoch 500, Cost: 0.15106629631618856\n",
      "Epoch 600, Cost: 0.14147042246695699\n",
      "Epoch 700, Cost: 0.13379707909572608\n",
      "Epoch 800, Cost: 0.12760038997719508\n",
      "Epoch 900, Cost: 0.12254593576522632\n",
      "Epoch 1000, Cost: 0.11838274401860538\n",
      "Epoch 1100, Cost: 0.11492157041481381\n",
      "Epoch 1200, Cost: 0.11201869600267272\n",
      "Epoch 1300, Cost: 0.10956405456299519\n",
      "Epoch 1400, Cost: 0.10747259099941976\n",
      "LR: 0.001  Epochs: 1500  -> Accuracy: 0.0806\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.24995630228840626\n",
      "Epoch 100, Cost: 0.21943872138193685\n",
      "Epoch 200, Cost: 0.19524527831056612\n",
      "Epoch 300, Cost: 0.17625162140065215\n",
      "Epoch 400, Cost: 0.16135896349951304\n",
      "Epoch 500, Cost: 0.14963206521375427\n",
      "Epoch 600, Cost: 0.14032841997843595\n",
      "Epoch 700, Cost: 0.13287907060757095\n",
      "Epoch 800, Cost: 0.12685513490631184\n",
      "Epoch 900, Cost: 0.12193502064260661\n",
      "Epoch 1000, Cost: 0.11787727202334711\n",
      "Epoch 1100, Cost: 0.11449966356948275\n",
      "Epoch 1200, Cost: 0.11166365613793167\n",
      "Epoch 1300, Cost: 0.1092630231859474\n",
      "Epoch 1400, Cost: 0.10721557494866114\n",
      "Epoch 1500, Cost: 0.10545713419457892\n",
      "Epoch 1600, Cost: 0.10393713343184348\n",
      "Epoch 1700, Cost: 0.10261537743093939\n",
      "Epoch 1800, Cost: 0.10145964513039912\n",
      "Epoch 1900, Cost: 0.10044389907480454\n",
      "LR: 0.001  Epochs: 2000  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2527648366089179\n",
      "Epoch 100, Cost: 0.22168474129197438\n",
      "Epoch 200, Cost: 0.19701491116593134\n",
      "Epoch 300, Cost: 0.17763930283324136\n",
      "Epoch 400, Cost: 0.1624493837128759\n",
      "Epoch 500, Cost: 0.15049425442437003\n",
      "Epoch 600, Cost: 0.1410159716986146\n",
      "Epoch 700, Cost: 0.1334326179263791\n",
      "Epoch 800, Cost: 0.12730519377184418\n",
      "Epoch 900, Cost: 0.12230448295609189\n",
      "Epoch 1000, Cost: 0.11818337544110051\n",
      "Epoch 1100, Cost: 0.11475547419978069\n",
      "Epoch 1200, Cost: 0.11187915928237809\n",
      "Epoch 1300, Cost: 0.1094459181954969\n",
      "Epoch 1400, Cost: 0.10737185342054988\n",
      "Epoch 1500, Cost: 0.1055915017042538\n",
      "Epoch 1600, Cost: 0.10405331879658482\n",
      "Epoch 1700, Cost: 0.10271636107474384\n",
      "Epoch 1800, Cost: 0.1015478290309602\n",
      "Epoch 1900, Cost: 0.10052123427969122\n",
      "Epoch 2000, Cost: 0.09961502048887981\n",
      "Epoch 2100, Cost: 0.09881151715687458\n",
      "Epoch 2200, Cost: 0.09809613933886653\n",
      "Epoch 2300, Cost: 0.09745677055709785\n",
      "Epoch 2400, Cost: 0.09688328323564487\n",
      "LR: 0.001  Epochs: 2500  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.25060485695429174\n",
      "Epoch 100, Cost: 0.14969074070569335\n",
      "Epoch 200, Cost: 0.11785534648112486\n",
      "Epoch 300, Cost: 0.10543134011021196\n",
      "Epoch 400, Cost: 0.09952578524727083\n",
      "LR: 0.005  Epochs: 500   -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2495861159583664\n",
      "Epoch 100, Cost: 0.14940197265847077\n",
      "Epoch 200, Cost: 0.11775874366613993\n",
      "Epoch 300, Cost: 0.10539156836107928\n",
      "Epoch 400, Cost: 0.0995071290018804\n",
      "Epoch 500, Cost: 0.09630334591431176\n",
      "Epoch 600, Cost: 0.09438918977578889\n",
      "Epoch 700, Cost: 0.09316669994856082\n",
      "Epoch 800, Cost: 0.09234616789921474\n",
      "Epoch 900, Cost: 0.0917739493090326\n",
      "LR: 0.005  Epochs: 1000  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.24952748390119633\n",
      "Epoch 100, Cost: 0.14937558738808118\n",
      "Epoch 200, Cost: 0.11774503898979011\n",
      "Epoch 300, Cost: 0.10538138607350973\n",
      "Epoch 400, Cost: 0.0994980879671024\n",
      "Epoch 500, Cost: 0.09629472630512952\n",
      "Epoch 600, Cost: 0.09438073516936134\n",
      "Epoch 700, Cost: 0.09315830520056277\n",
      "Epoch 800, Cost: 0.0923377836780207\n",
      "Epoch 900, Cost: 0.09176554885155105\n",
      "Epoch 1000, Cost: 0.09135420633484706\n",
      "Epoch 1100, Cost: 0.09105118878692031\n",
      "Epoch 1200, Cost: 0.09082341493086192\n",
      "Epoch 1300, Cost: 0.09064927463663658\n",
      "Epoch 1400, Cost: 0.09051420282362019\n",
      "LR: 0.005  Epochs: 1500  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2511728866920976\n",
      "Epoch 100, Cost: 0.14988053881542193\n",
      "Epoch 200, Cost: 0.11793069409072118\n",
      "Epoch 300, Cost: 0.10546912924476697\n",
      "Epoch 400, Cost: 0.09954832659529632\n",
      "Epoch 500, Cost: 0.09632793628151753\n",
      "Epoch 600, Cost: 0.09440521743888922\n",
      "Epoch 700, Cost: 0.09317790790487634\n",
      "Epoch 800, Cost: 0.09235448173551936\n",
      "Epoch 900, Cost: 0.09178043957841059\n",
      "Epoch 1000, Cost: 0.09136793965135843\n",
      "Epoch 1100, Cost: 0.09106417054647867\n",
      "Epoch 1200, Cost: 0.09083591025539228\n",
      "Epoch 1300, Cost: 0.09066146367339245\n",
      "Epoch 1400, Cost: 0.09052621237809284\n",
      "Epoch 1500, Cost: 0.09042004106965319\n",
      "Epoch 1600, Cost: 0.09033578911036703\n",
      "Epoch 1700, Cost: 0.09026828653390691\n",
      "Epoch 1800, Cost: 0.09021373587300763\n",
      "Epoch 1900, Cost: 0.09016930532364825\n",
      "LR: 0.005  Epochs: 2000  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2472006802735107\n",
      "Epoch 100, Cost: 0.14866170257412378\n",
      "Epoch 200, Cost: 0.1174909223472337\n",
      "Epoch 300, Cost: 0.10526903873751611\n",
      "Epoch 400, Cost: 0.09944025720578296\n",
      "Epoch 500, Cost: 0.09626171839078464\n",
      "Epoch 600, Cost: 0.0943604589604649\n",
      "Epoch 700, Cost: 0.09314513157264064\n",
      "Epoch 800, Cost: 0.09232881448918169\n",
      "Epoch 900, Cost: 0.09175917189468903\n",
      "Epoch 1000, Cost: 0.09134946829504909\n",
      "Epoch 1100, Cost: 0.09104749749208275\n",
      "Epoch 1200, Cost: 0.09082038729341395\n",
      "Epoch 1300, Cost: 0.0906466542133535\n",
      "Epoch 1400, Cost: 0.09051181283280188\n",
      "Epoch 1500, Cost: 0.09040583419644153\n",
      "Epoch 1600, Cost: 0.09032161529304134\n",
      "Epoch 1700, Cost: 0.09025402554055426\n",
      "Epoch 1800, Cost: 0.0901992949436897\n",
      "Epoch 1900, Cost: 0.09015461119941261\n",
      "Epoch 2000, Cost: 0.09011784825983596\n",
      "Epoch 2100, Cost: 0.09008737970637978\n",
      "Epoch 2200, Cost: 0.09006194808271001\n",
      "Epoch 2300, Cost: 0.09004057190073507\n",
      "Epoch 2400, Cost: 0.0900224784743413\n",
      "LR: 0.005  Epochs: 2500  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.24989486910117306\n",
      "Epoch 100, Cost: 0.11767274302138135\n",
      "Epoch 200, Cost: 0.09946173843362244\n",
      "Epoch 300, Cost: 0.09436059203987539\n",
      "Epoch 400, Cost: 0.09232438286864655\n",
      "LR: 0.01   Epochs: 500   -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.25409375390133954\n",
      "Epoch 100, Cost: 0.11814250061328312\n",
      "Epoch 200, Cost: 0.09957703949595453\n",
      "Epoch 300, Cost: 0.09440793690872823\n",
      "Epoch 400, Cost: 0.09235087815219878\n",
      "Epoch 500, Cost: 0.09136215215262464\n",
      "Epoch 600, Cost: 0.0908290887001723\n",
      "Epoch 700, Cost: 0.09051872490888459\n",
      "Epoch 800, Cost: 0.09032775583051922\n",
      "Epoch 900, Cost: 0.09020518717113567\n",
      "LR: 0.01   Epochs: 1000  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2478061557856251\n",
      "Epoch 100, Cost: 0.11746471714678479\n",
      "Epoch 200, Cost: 0.0994229687557254\n",
      "Epoch 300, Cost: 0.09435320668462782\n",
      "Epoch 400, Cost: 0.09232658028548288\n",
      "Epoch 500, Cost: 0.09135000069491697\n",
      "Epoch 600, Cost: 0.09082266263597653\n",
      "Epoch 700, Cost: 0.09051533247351183\n",
      "Epoch 800, Cost: 0.09032612384193475\n",
      "Epoch 900, Cost: 0.0902046593382356\n",
      "Epoch 1000, Cost: 0.09012400156150656\n",
      "Epoch 1100, Cost: 0.09006886146330632\n",
      "Epoch 1200, Cost: 0.09003014642025238\n",
      "Epoch 1300, Cost: 0.09000224302213222\n",
      "Epoch 1400, Cost: 0.08998157865262323\n",
      "LR: 0.01   Epochs: 1500  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.24849134256952535\n",
      "Epoch 100, Cost: 0.11753830876553473\n",
      "Epoch 200, Cost: 0.09944031609489752\n",
      "Epoch 300, Cost: 0.09435988467424837\n",
      "Epoch 400, Cost: 0.09232999925751617\n",
      "Epoch 500, Cost: 0.0913521025083753\n",
      "Epoch 600, Cost: 0.09082411305953425\n",
      "Epoch 700, Cost: 0.09051640012372211\n",
      "Epoch 800, Cost: 0.09032692810442046\n",
      "Epoch 900, Cost: 0.0902052554885277\n",
      "Epoch 1000, Cost: 0.09012441436564035\n",
      "Epoch 1100, Cost: 0.09006909988593324\n",
      "Epoch 1200, Cost: 0.09003021058615035\n",
      "Epoch 1300, Cost: 0.09000212770946409\n",
      "Epoch 1400, Cost: 0.08998127511866727\n",
      "Epoch 1500, Cost: 0.08996531842249715\n",
      "Epoch 1600, Cost: 0.08995270089582738\n",
      "Epoch 1700, Cost: 0.08994236509255096\n",
      "Epoch 1800, Cost: 0.0899335808668346\n",
      "Epoch 1900, Cost: 0.08992583642169816\n",
      "LR: 0.01   Epochs: 2000  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2547296351783868\n",
      "Epoch 100, Cost: 0.11822256620639388\n",
      "Epoch 200, Cost: 0.0996003155481926\n",
      "Epoch 300, Cost: 0.09441892215161479\n",
      "Epoch 400, Cost: 0.092357640756511\n",
      "Epoch 500, Cost: 0.09136711979130051\n",
      "Epoch 600, Cost: 0.09083322626273556\n",
      "Epoch 700, Cost: 0.09052248961297915\n",
      "Epoch 800, Cost: 0.09033139301943727\n",
      "Epoch 900, Cost: 0.09020884184640458\n",
      "Epoch 1000, Cost: 0.09012755046895683\n",
      "Epoch 1100, Cost: 0.09007204751726614\n",
      "Epoch 1200, Cost: 0.09003313908296023\n",
      "Epoch 1300, Cost: 0.09000515276435549\n",
      "Epoch 1400, Cost: 0.08998448025361509\n",
      "Epoch 1500, Cost: 0.08996876744010349\n",
      "Epoch 1600, Cost: 0.08995644539647896\n",
      "Epoch 1700, Cost: 0.0899464491798732\n",
      "Epoch 1800, Cost: 0.08993804419375723\n",
      "Epoch 1900, Cost: 0.0899307162275703\n",
      "Epoch 2000, Cost: 0.08992410029174003\n",
      "Epoch 2100, Cost: 0.08991793368556217\n",
      "Epoch 2200, Cost: 0.08991202453497414\n",
      "Epoch 2300, Cost: 0.08990623039620016\n",
      "Epoch 2400, Cost: 0.0899004435194423\n",
      "LR: 0.01   Epochs: 2500  -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2507399138575796\n",
      "Epoch 100, Cost: 0.09131826626785837\n",
      "Epoch 200, Cost: 0.09012290889409144\n",
      "Epoch 300, Cost: 0.08996776067961154\n",
      "Epoch 400, Cost: 0.08992233932318207\n",
      "LR: 0.05   Epochs: 500   -> Accuracy: 0.0778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.25036879605287116\n",
      "Epoch 100, Cost: 0.09131421938998872\n",
      "Epoch 200, Cost: 0.09012203404288933\n",
      "Epoch 300, Cost: 0.089968927906728\n",
      "Epoch 400, Cost: 0.08992557545785083\n",
      "Epoch 500, Cost: 0.08989647479421937\n",
      "Epoch 600, Cost: 0.08986412736847657\n",
      "Epoch 700, Cost: 0.0898227612344328\n",
      "Epoch 800, Cost: 0.0897680553487138\n",
      "Epoch 900, Cost: 0.08969486842373256\n",
      "LR: 0.05   Epochs: 1000  -> Accuracy: 0.1111\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2495265288834819\n",
      "Epoch 100, Cost: 0.09130824110883495\n",
      "Epoch 200, Cost: 0.09011523178713726\n",
      "Epoch 300, Cost: 0.08995975652550857\n",
      "Epoch 400, Cost: 0.08991336252647768\n",
      "Epoch 500, Cost: 0.08988032675640428\n",
      "Epoch 600, Cost: 0.08984279085820597\n",
      "Epoch 700, Cost: 0.0897944941103987\n",
      "Epoch 800, Cost: 0.0897304471306317\n",
      "Epoch 900, Cost: 0.08964458386386372\n",
      "Epoch 1000, Cost: 0.08952872690645368\n",
      "Epoch 1100, Cost: 0.08937160267515407\n",
      "Epoch 1200, Cost: 0.0891575547307372\n",
      "Epoch 1300, Cost: 0.08886475940602413\n",
      "Epoch 1400, Cost: 0.08846278746256249\n",
      "LR: 0.05   Epochs: 1500  -> Accuracy: 0.5361\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2526184165962445\n",
      "Epoch 100, Cost: 0.0913191280157642\n",
      "Epoch 200, Cost: 0.09011878738571963\n",
      "Epoch 300, Cost: 0.0899624646316\n",
      "Epoch 400, Cost: 0.08991620589994098\n",
      "Epoch 500, Cost: 0.08988398145614537\n",
      "Epoch 600, Cost: 0.08984811617648869\n",
      "Epoch 700, Cost: 0.08980272395973408\n",
      "Epoch 800, Cost: 0.08974338698259204\n",
      "Epoch 900, Cost: 0.08966487537466514\n",
      "Epoch 1000, Cost: 0.08956021567228896\n",
      "Epoch 1100, Cost: 0.08941984342579057\n",
      "Epoch 1200, Cost: 0.08923048534754173\n",
      "Epoch 1300, Cost: 0.08897355021632923\n",
      "Epoch 1400, Cost: 0.08862277540019468\n",
      "Epoch 1500, Cost: 0.08814082585317129\n",
      "Epoch 1600, Cost: 0.08747464298402821\n",
      "Epoch 1700, Cost: 0.08655008567097418\n",
      "Epoch 1800, Cost: 0.08526908346198171\n",
      "Epoch 1900, Cost: 0.08351895864137267\n",
      "LR: 0.05   Epochs: 2000  -> Accuracy: 0.5417\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.25181456956499737\n",
      "Epoch 100, Cost: 0.09132167713979567\n",
      "Epoch 200, Cost: 0.09012610311306275\n",
      "Epoch 300, Cost: 0.08997345281958276\n",
      "Epoch 400, Cost: 0.08993158808258565\n",
      "Epoch 500, Cost: 0.0899049075556834\n",
      "Epoch 600, Cost: 0.08987615820535196\n",
      "Epoch 700, Cost: 0.08984000323617419\n",
      "Epoch 800, Cost: 0.08979276611007005\n",
      "Epoch 900, Cost: 0.08973022996459783\n",
      "Epoch 1000, Cost: 0.08964682450280001\n",
      "Epoch 1100, Cost: 0.08953496043365426\n",
      "Epoch 1200, Cost: 0.08938419527706347\n",
      "Epoch 1300, Cost: 0.08918007030228545\n",
      "Epoch 1400, Cost: 0.08890245333011057\n",
      "Epoch 1500, Cost: 0.08852318521524376\n",
      "Epoch 1600, Cost: 0.08800284804521967\n",
      "Epoch 1700, Cost: 0.08728678421480722\n",
      "Epoch 1800, Cost: 0.08630172382941317\n",
      "Epoch 1900, Cost: 0.08495760199045224\n",
      "Epoch 2000, Cost: 0.08316375682398058\n",
      "Epoch 2100, Cost: 0.08086527131242915\n",
      "Epoch 2200, Cost: 0.07808096957974328\n",
      "Epoch 2300, Cost: 0.07490897008552529\n",
      "Epoch 2400, Cost: 0.07149847532478672\n",
      "LR: 0.05   Epochs: 2500  -> Accuracy: 0.6083\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.25053179241188117\n",
      "Epoch 100, Cost: 0.09011879389847091\n",
      "Epoch 200, Cost: 0.08992692294618054\n",
      "Epoch 300, Cost: 0.0898664979703094\n",
      "Epoch 400, Cost: 0.08977489978735485\n",
      "LR: 0.1    Epochs: 500   -> Accuracy: 0.0889\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2479074127110812\n",
      "Epoch 100, Cost: 0.0901158166090699\n",
      "Epoch 200, Cost: 0.08992366233667341\n",
      "Epoch 300, Cost: 0.08986055227535023\n",
      "Epoch 400, Cost: 0.08976308329783671\n",
      "Epoch 500, Cost: 0.08959235959766498\n",
      "Epoch 600, Cost: 0.08928702576060255\n",
      "Epoch 700, Cost: 0.08873447931853276\n",
      "Epoch 800, Cost: 0.08772565647083422\n",
      "Epoch 900, Cost: 0.08588539674206973\n",
      "LR: 0.1    Epochs: 1000  -> Accuracy: 0.5222\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.24882396145528923\n",
      "Epoch 100, Cost: 0.09010430914737588\n",
      "Epoch 200, Cost: 0.08990896952894013\n",
      "Epoch 300, Cost: 0.08983945183620841\n",
      "Epoch 400, Cost: 0.08973037747664968\n",
      "Epoch 500, Cost: 0.08953812879239291\n",
      "Epoch 600, Cost: 0.08919004263315851\n",
      "Epoch 700, Cost: 0.08854505805113454\n",
      "Epoch 800, Cost: 0.08732184030349961\n",
      "Epoch 900, Cost: 0.08497938714354857\n",
      "Epoch 1000, Cost: 0.08076130207197389\n",
      "Epoch 1100, Cost: 0.07459489218494995\n",
      "Epoch 1200, Cost: 0.06775474860282393\n",
      "Epoch 1300, Cost: 0.061505038531633686\n",
      "Epoch 1400, Cost: 0.0561905389235981\n",
      "LR: 0.1    Epochs: 1500  -> Accuracy: 0.7111\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2532573909845577\n",
      "Epoch 100, Cost: 0.09011188146533276\n",
      "Epoch 200, Cost: 0.08991877451829924\n",
      "Epoch 300, Cost: 0.08985587216987277\n",
      "Epoch 400, Cost: 0.08975947210004835\n",
      "Epoch 500, Cost: 0.08959122329923362\n",
      "Epoch 600, Cost: 0.08928909883686469\n",
      "Epoch 700, Cost: 0.08873335437654715\n",
      "Epoch 800, Cost: 0.08768348473058386\n",
      "Epoch 900, Cost: 0.08565603296915987\n",
      "Epoch 1000, Cost: 0.08186053484520754\n",
      "Epoch 1100, Cost: 0.07587873004631943\n",
      "Epoch 1200, Cost: 0.06867695525426633\n",
      "Epoch 1300, Cost: 0.061889594069700904\n",
      "Epoch 1400, Cost: 0.056297209106378365\n",
      "Epoch 1500, Cost: 0.051786614824632184\n",
      "Epoch 1600, Cost: 0.04801069161020234\n",
      "Epoch 1700, Cost: 0.0446447966931413\n",
      "Epoch 1800, Cost: 0.0414729739904823\n",
      "Epoch 1900, Cost: 0.03846452557507428\n",
      "LR: 0.1    Epochs: 2000  -> Accuracy: 0.8361\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.2518640330429143\n",
      "Epoch 100, Cost: 0.09010379957417483\n",
      "Epoch 200, Cost: 0.08990148009260635\n",
      "Epoch 300, Cost: 0.08982144467553303\n",
      "Epoch 400, Cost: 0.08969327165043496\n",
      "Epoch 500, Cost: 0.08946422364535664\n",
      "Epoch 600, Cost: 0.08904236136781242\n",
      "Epoch 700, Cost: 0.08824464425696067\n",
      "Epoch 800, Cost: 0.08670599495514188\n",
      "Epoch 900, Cost: 0.08382464013009479\n",
      "Epoch 1000, Cost: 0.07928672528826332\n",
      "Epoch 1100, Cost: 0.07379499041466227\n",
      "Epoch 1200, Cost: 0.06801091543570927\n",
      "Epoch 1300, Cost: 0.06221716258645424\n",
      "Epoch 1400, Cost: 0.05690960206760713\n",
      "Epoch 1500, Cost: 0.0523743194823672\n",
      "Epoch 1600, Cost: 0.04848681281336695\n",
      "Epoch 1700, Cost: 0.04500835177957859\n",
      "Epoch 1800, Cost: 0.041755084167941985\n",
      "Epoch 1900, Cost: 0.03868491872405041\n",
      "Epoch 2000, Cost: 0.03587327130986508\n",
      "Epoch 2100, Cost: 0.03337813945086126\n",
      "Epoch 2200, Cost: 0.03119025165507142\n",
      "Epoch 2300, Cost: 0.02926817476753521\n",
      "Epoch 2400, Cost: 0.027568380634427236\n",
      "LR: 0.1    Epochs: 2500  -> Accuracy: 0.8806\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Experimenting with ReLU activation ===\n",
      "\n",
      "Epoch 0, Cost: 0.09965742890584883\n",
      "Epoch 100, Cost: 0.09655212104033388\n",
      "Epoch 200, Cost: 0.0941287094887212\n",
      "Epoch 300, Cost: 0.09246186313552228\n",
      "Epoch 400, Cost: 0.09129183982700655\n",
      "LR: 0.001  Epochs: 500   -> Accuracy: 0.3167\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09988179293576317\n",
      "Epoch 100, Cost: 0.09722943828627052\n",
      "Epoch 200, Cost: 0.09492058175432178\n",
      "Epoch 300, Cost: 0.09307080207446859\n",
      "Epoch 400, Cost: 0.0917908603752843\n",
      "Epoch 500, Cost: 0.09088394771657883\n",
      "Epoch 600, Cost: 0.09021758191688932\n",
      "Epoch 700, Cost: 0.0897016475577097\n",
      "Epoch 800, Cost: 0.08927543618514468\n",
      "Epoch 900, Cost: 0.08889651061624254\n",
      "LR: 0.001  Epochs: 1000  -> Accuracy: 0.3444\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09983752504170117\n",
      "Epoch 100, Cost: 0.09737987045234484\n",
      "Epoch 200, Cost: 0.09517258678139709\n",
      "Epoch 300, Cost: 0.09359630064561715\n",
      "Epoch 400, Cost: 0.09250245377623495\n",
      "Epoch 500, Cost: 0.09172330749963939\n",
      "Epoch 600, Cost: 0.09114666050820644\n",
      "Epoch 700, Cost: 0.09069653903686767\n",
      "Epoch 800, Cost: 0.09032108126027419\n",
      "Epoch 900, Cost: 0.0899834290178007\n",
      "Epoch 1000, Cost: 0.08965765127008478\n",
      "Epoch 1100, Cost: 0.08932494422154162\n",
      "Epoch 1200, Cost: 0.08896980160480947\n",
      "Epoch 1300, Cost: 0.08857967198251583\n",
      "Epoch 1400, Cost: 0.08814420294673349\n",
      "LR: 0.001  Epochs: 1500  -> Accuracy: 0.6417\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09979793425597777\n",
      "Epoch 100, Cost: 0.09732399486151788\n",
      "Epoch 200, Cost: 0.0950109281524071\n",
      "Epoch 300, Cost: 0.09328434257177587\n",
      "Epoch 400, Cost: 0.09189164291407739\n",
      "Epoch 500, Cost: 0.09090442547633873\n",
      "Epoch 600, Cost: 0.09017740458762297\n",
      "Epoch 700, Cost: 0.0896131432381544\n",
      "Epoch 800, Cost: 0.08914537700519992\n",
      "Epoch 900, Cost: 0.08872733275637153\n",
      "Epoch 1000, Cost: 0.08832665144932401\n",
      "Epoch 1100, Cost: 0.08791964727052765\n",
      "Epoch 1200, Cost: 0.08748796590511887\n",
      "Epoch 1300, Cost: 0.08701656122982941\n",
      "Epoch 1400, Cost: 0.08649248384087442\n",
      "Epoch 1500, Cost: 0.08590419866976988\n",
      "Epoch 1600, Cost: 0.08524123966033982\n",
      "Epoch 1700, Cost: 0.08449401725574297\n",
      "Epoch 1800, Cost: 0.08365384563506179\n",
      "Epoch 1900, Cost: 0.08271260271861254\n",
      "LR: 0.001  Epochs: 2000  -> Accuracy: 0.6389\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.0996849973702272\n",
      "Epoch 100, Cost: 0.09668065347354972\n",
      "Epoch 200, Cost: 0.09420114915899232\n",
      "Epoch 300, Cost: 0.09249514526548004\n",
      "Epoch 400, Cost: 0.09129487293558873\n",
      "Epoch 500, Cost: 0.09042215746081284\n",
      "Epoch 600, Cost: 0.0897566115384684\n",
      "Epoch 700, Cost: 0.08921672178559992\n",
      "Epoch 800, Cost: 0.08874694317296225\n",
      "Epoch 900, Cost: 0.08830757369174222\n",
      "Epoch 1000, Cost: 0.08786975333445442\n",
      "Epoch 1100, Cost: 0.08741285686899282\n",
      "Epoch 1200, Cost: 0.08691936477130364\n",
      "Epoch 1300, Cost: 0.08637516913332467\n",
      "Epoch 1400, Cost: 0.08576769245506077\n",
      "Epoch 1500, Cost: 0.08508535322050076\n",
      "Epoch 1600, Cost: 0.08431845741281566\n",
      "Epoch 1700, Cost: 0.08345800134945719\n",
      "Epoch 1800, Cost: 0.08249739947988598\n",
      "Epoch 1900, Cost: 0.08143153543899437\n",
      "Epoch 2000, Cost: 0.08025771257720361\n",
      "Epoch 2100, Cost: 0.07897706345334903\n",
      "Epoch 2200, Cost: 0.07759264942535533\n",
      "Epoch 2300, Cost: 0.07611189428887002\n",
      "Epoch 2400, Cost: 0.07454489333918948\n",
      "LR: 0.001  Epochs: 2500  -> Accuracy: 0.6667\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.099825670418594\n",
      "Epoch 100, Cost: 0.09105138347261137\n",
      "Epoch 200, Cost: 0.08815601969237061\n",
      "Epoch 300, Cost: 0.08544467614634062\n",
      "Epoch 400, Cost: 0.08068370792673\n",
      "LR: 0.005  Epochs: 500   -> Accuracy: 0.6472\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09968920183075618\n",
      "Epoch 100, Cost: 0.09043292950394279\n",
      "Epoch 200, Cost: 0.08791112930930656\n",
      "Epoch 300, Cost: 0.08516291198585475\n",
      "Epoch 400, Cost: 0.08037761059408378\n",
      "Epoch 500, Cost: 0.0730858368148919\n",
      "Epoch 600, Cost: 0.06421390505794838\n",
      "Epoch 700, Cost: 0.05512131644250018\n",
      "Epoch 800, Cost: 0.04698187847680252\n",
      "Epoch 900, Cost: 0.04033665514054353\n",
      "LR: 0.005  Epochs: 1000  -> Accuracy: 0.8333\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.0997864608088236\n",
      "Epoch 100, Cost: 0.09165422123189271\n",
      "Epoch 200, Cost: 0.08937626740228286\n",
      "Epoch 300, Cost: 0.08704714131408547\n",
      "Epoch 400, Cost: 0.0828473852823657\n",
      "Epoch 500, Cost: 0.07586911441956837\n",
      "Epoch 600, Cost: 0.06591855092219472\n",
      "Epoch 700, Cost: 0.05640454146344365\n",
      "Epoch 800, Cost: 0.048038741875502416\n",
      "Epoch 900, Cost: 0.04124119725513826\n",
      "Epoch 1000, Cost: 0.036003451579458785\n",
      "Epoch 1100, Cost: 0.03207270359675073\n",
      "Epoch 1200, Cost: 0.029066898279371613\n",
      "Epoch 1300, Cost: 0.026650203418085248\n",
      "Epoch 1400, Cost: 0.02461170821547948\n",
      "LR: 0.005  Epochs: 1500  -> Accuracy: 0.9139\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09983483796376856\n",
      "Epoch 100, Cost: 0.09072911512318368\n",
      "Epoch 200, Cost: 0.08841097665629626\n",
      "Epoch 300, Cost: 0.08607381925090424\n",
      "Epoch 400, Cost: 0.08192140031028697\n",
      "Epoch 500, Cost: 0.07522534573754065\n",
      "Epoch 600, Cost: 0.06664122430918354\n",
      "Epoch 700, Cost: 0.057591384461109035\n",
      "Epoch 800, Cost: 0.0491476434438785\n",
      "Epoch 900, Cost: 0.04203771925493481\n",
      "Epoch 1000, Cost: 0.03653128499201151\n",
      "Epoch 1100, Cost: 0.03238520931702525\n",
      "Epoch 1200, Cost: 0.029184815919398574\n",
      "Epoch 1300, Cost: 0.02657446157824367\n",
      "Epoch 1400, Cost: 0.024341598856962\n",
      "Epoch 1500, Cost: 0.02236904717577737\n",
      "Epoch 1600, Cost: 0.020618053119480256\n",
      "Epoch 1700, Cost: 0.01906578281529422\n",
      "Epoch 1800, Cost: 0.017701460299770207\n",
      "Epoch 1900, Cost: 0.01650985165640315\n",
      "LR: 0.005  Epochs: 2000  -> Accuracy: 0.9389\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09983931097437461\n",
      "Epoch 100, Cost: 0.0905973021708051\n",
      "Epoch 200, Cost: 0.0880982873421627\n",
      "Epoch 300, Cost: 0.08546663179498246\n",
      "Epoch 400, Cost: 0.08077996069552569\n",
      "Epoch 500, Cost: 0.073480267292213\n",
      "Epoch 600, Cost: 0.06496085327956047\n",
      "Epoch 700, Cost: 0.05705026808243143\n",
      "Epoch 800, Cost: 0.0502801669605742\n",
      "Epoch 900, Cost: 0.0445893005386403\n",
      "Epoch 1000, Cost: 0.0396920595955931\n",
      "Epoch 1100, Cost: 0.03530472426296806\n",
      "Epoch 1200, Cost: 0.031352361345030916\n",
      "Epoch 1300, Cost: 0.027886834269313706\n",
      "Epoch 1400, Cost: 0.024931677492592674\n",
      "Epoch 1500, Cost: 0.022465650083423366\n",
      "Epoch 1600, Cost: 0.020433253436192667\n",
      "Epoch 1700, Cost: 0.01875043762883724\n",
      "Epoch 1800, Cost: 0.017338474014791064\n",
      "Epoch 1900, Cost: 0.0161420834351914\n",
      "Epoch 2000, Cost: 0.015121849577282853\n",
      "Epoch 2100, Cost: 0.014246185018960307\n",
      "Epoch 2200, Cost: 0.013493414956114353\n",
      "Epoch 2300, Cost: 0.012845760514348884\n",
      "Epoch 2400, Cost: 0.012284787529473144\n",
      "LR: 0.005  Epochs: 2500  -> Accuracy: 0.9472\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09975620433892468\n",
      "Epoch 100, Cost: 0.08834239529567404\n",
      "Epoch 200, Cost: 0.08195217138375481\n",
      "Epoch 300, Cost: 0.06773246476848019\n",
      "Epoch 400, Cost: 0.051291592054894954\n",
      "LR: 0.01   Epochs: 500   -> Accuracy: 0.8250\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09983701180921753\n",
      "Epoch 100, Cost: 0.08951246074674025\n",
      "Epoch 200, Cost: 0.082994225905599\n",
      "Epoch 300, Cost: 0.06901692496179117\n",
      "Epoch 400, Cost: 0.05354557340017931\n",
      "Epoch 500, Cost: 0.04145729475089057\n",
      "Epoch 600, Cost: 0.033074892889799735\n",
      "Epoch 700, Cost: 0.026753803279805363\n",
      "Epoch 800, Cost: 0.021847224931549048\n",
      "Epoch 900, Cost: 0.018307403635199317\n",
      "LR: 0.01   Epochs: 1000  -> Accuracy: 0.9361\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09971750644907999\n",
      "Epoch 100, Cost: 0.08800176722268645\n",
      "Epoch 200, Cost: 0.0808045756805189\n",
      "Epoch 300, Cost: 0.06447410231032677\n",
      "Epoch 400, Cost: 0.04761809118187167\n",
      "Epoch 500, Cost: 0.03503430518895548\n",
      "Epoch 600, Cost: 0.026720402318485013\n",
      "Epoch 700, Cost: 0.021247386563017077\n",
      "Epoch 800, Cost: 0.017592747990786976\n",
      "Epoch 900, Cost: 0.015179684583254164\n",
      "Epoch 1000, Cost: 0.013558143196090418\n",
      "Epoch 1100, Cost: 0.012415976258062088\n",
      "Epoch 1200, Cost: 0.011557995302469582\n",
      "Epoch 1300, Cost: 0.010891441551124736\n",
      "Epoch 1400, Cost: 0.010355670914076229\n",
      "LR: 0.01   Epochs: 1500  -> Accuracy: 0.9500\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09962213469771475\n",
      "Epoch 100, Cost: 0.08857597692719796\n",
      "Epoch 200, Cost: 0.08049203792402551\n",
      "Epoch 300, Cost: 0.06476123449109307\n",
      "Epoch 400, Cost: 0.04948788281139438\n",
      "Epoch 500, Cost: 0.03811509330967438\n",
      "Epoch 600, Cost: 0.031140681683005354\n",
      "Epoch 700, Cost: 0.026337955849419187\n",
      "Epoch 800, Cost: 0.022918209672449123\n",
      "Epoch 900, Cost: 0.02005722610120697\n",
      "Epoch 1000, Cost: 0.017437264399593123\n",
      "Epoch 1100, Cost: 0.015186556783478435\n",
      "Epoch 1200, Cost: 0.013426428217421373\n",
      "Epoch 1300, Cost: 0.01212034913122707\n",
      "Epoch 1400, Cost: 0.01115776780883418\n",
      "Epoch 1500, Cost: 0.010433943718805364\n",
      "Epoch 1600, Cost: 0.009866817134995298\n",
      "Epoch 1700, Cost: 0.009404180722948586\n",
      "Epoch 1800, Cost: 0.009019070480325048\n",
      "Epoch 1900, Cost: 0.00869136053177257\n",
      "LR: 0.01   Epochs: 2000  -> Accuracy: 0.9639\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09985161275186562\n",
      "Epoch 100, Cost: 0.08823390690741015\n",
      "Epoch 200, Cost: 0.08146132907328275\n",
      "Epoch 300, Cost: 0.06604867100439135\n",
      "Epoch 400, Cost: 0.04843185962667258\n",
      "Epoch 500, Cost: 0.03596311591660708\n",
      "Epoch 600, Cost: 0.02862884393532136\n",
      "Epoch 700, Cost: 0.02368266885738071\n",
      "Epoch 800, Cost: 0.019945478157034225\n",
      "Epoch 900, Cost: 0.017133927382657056\n",
      "Epoch 1000, Cost: 0.015032872902742243\n",
      "Epoch 1100, Cost: 0.013452081071069995\n",
      "Epoch 1200, Cost: 0.012251875456504201\n",
      "Epoch 1300, Cost: 0.011342172497230662\n",
      "Epoch 1400, Cost: 0.010640074950589055\n",
      "Epoch 1500, Cost: 0.010083530296418508\n",
      "Epoch 1600, Cost: 0.009626524061829088\n",
      "Epoch 1700, Cost: 0.00923755420098256\n",
      "Epoch 1800, Cost: 0.008902274963208705\n",
      "Epoch 1900, Cost: 0.008604686617012141\n",
      "Epoch 2000, Cost: 0.008339530507616696\n",
      "Epoch 2100, Cost: 0.008100964020683362\n",
      "Epoch 2200, Cost: 0.00788347232356697\n",
      "Epoch 2300, Cost: 0.007683060958634055\n",
      "Epoch 2400, Cost: 0.007498230014750322\n",
      "LR: 0.01   Epochs: 2500  -> Accuracy: 0.9778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09987902462925276\n",
      "Epoch 100, Cost: 0.04367858660746812\n",
      "Epoch 200, Cost: 0.016758081375755535\n",
      "Epoch 300, Cost: 0.010538129868979755\n",
      "Epoch 400, Cost: 0.008460732878606731\n",
      "LR: 0.05   Epochs: 500   -> Accuracy: 0.9778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.0998838653282412\n",
      "Epoch 100, Cost: 0.0373411018920553\n",
      "Epoch 200, Cost: 0.015023053198488776\n",
      "Epoch 300, Cost: 0.00985308715510714\n",
      "Epoch 400, Cost: 0.008054644509248468\n",
      "Epoch 500, Cost: 0.0070375474161042985\n",
      "Epoch 600, Cost: 0.006323882411997271\n",
      "Epoch 700, Cost: 0.005765746326243088\n",
      "Epoch 800, Cost: 0.005329863704886647\n",
      "Epoch 900, Cost: 0.004954266729233341\n",
      "LR: 0.05   Epochs: 1000  -> Accuracy: 0.9806\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.0997928018124128\n",
      "Epoch 100, Cost: 0.046758372145352514\n",
      "Epoch 200, Cost: 0.016518771049422766\n",
      "Epoch 300, Cost: 0.01027232609771668\n",
      "Epoch 400, Cost: 0.008204627388692404\n",
      "Epoch 500, Cost: 0.007147935544167105\n",
      "Epoch 600, Cost: 0.006437761973963194\n",
      "Epoch 700, Cost: 0.00590360665613022\n",
      "Epoch 800, Cost: 0.005475227363312489\n",
      "Epoch 900, Cost: 0.005114315158065869\n",
      "Epoch 1000, Cost: 0.0048064920914421844\n",
      "Epoch 1100, Cost: 0.00453856344983061\n",
      "Epoch 1200, Cost: 0.0042981947989602035\n",
      "Epoch 1300, Cost: 0.004044150545249705\n",
      "Epoch 1400, Cost: 0.003824629902078375\n",
      "LR: 0.05   Epochs: 1500  -> Accuracy: 0.9833\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09983710067049291\n",
      "Epoch 100, Cost: 0.03488553828662286\n",
      "Epoch 200, Cost: 0.014339785485374984\n",
      "Epoch 300, Cost: 0.01000341461648915\n",
      "Epoch 400, Cost: 0.008426022201112753\n",
      "Epoch 500, Cost: 0.007486567920358447\n",
      "Epoch 600, Cost: 0.006817851008353804\n",
      "Epoch 700, Cost: 0.006306017123165255\n",
      "Epoch 800, Cost: 0.005892968366168281\n",
      "Epoch 900, Cost: 0.005512480801647767\n",
      "Epoch 1000, Cost: 0.005165646055826501\n",
      "Epoch 1100, Cost: 0.004869310981317472\n",
      "Epoch 1200, Cost: 0.004608170478330167\n",
      "Epoch 1300, Cost: 0.004371233234208458\n",
      "Epoch 1400, Cost: 0.004155050725393053\n",
      "Epoch 1500, Cost: 0.00396052272853288\n",
      "Epoch 1600, Cost: 0.003782277767244143\n",
      "Epoch 1700, Cost: 0.003617228536728332\n",
      "Epoch 1800, Cost: 0.003462850362455589\n",
      "Epoch 1900, Cost: 0.0033209497203317423\n",
      "LR: 0.05   Epochs: 2000  -> Accuracy: 0.9806\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09988799739441498\n",
      "Epoch 100, Cost: 0.038981239753699784\n",
      "Epoch 200, Cost: 0.015139269295922705\n",
      "Epoch 300, Cost: 0.010049971266103129\n",
      "Epoch 400, Cost: 0.008287697608848803\n",
      "Epoch 500, Cost: 0.007292416296389964\n",
      "Epoch 600, Cost: 0.006619655025940749\n",
      "Epoch 700, Cost: 0.006115338604501801\n",
      "Epoch 800, Cost: 0.005719349386287451\n",
      "Epoch 900, Cost: 0.005394940024491772\n",
      "Epoch 1000, Cost: 0.00511774767446024\n",
      "Epoch 1100, Cost: 0.0048732958465757875\n",
      "Epoch 1200, Cost: 0.004648359984865352\n",
      "Epoch 1300, Cost: 0.004403729659281803\n",
      "Epoch 1400, Cost: 0.004199154214745732\n",
      "Epoch 1500, Cost: 0.004019362823559015\n",
      "Epoch 1600, Cost: 0.0038551246693316046\n",
      "Epoch 1700, Cost: 0.0037027994195419057\n",
      "Epoch 1800, Cost: 0.0035604045400474188\n",
      "Epoch 1900, Cost: 0.003427505088425564\n",
      "Epoch 2000, Cost: 0.0033034635649996547\n",
      "Epoch 2100, Cost: 0.003187051110714715\n",
      "Epoch 2200, Cost: 0.0030779339549419555\n",
      "Epoch 2300, Cost: 0.002975342124613359\n",
      "Epoch 2400, Cost: 0.0028788284571474474\n",
      "LR: 0.05   Epochs: 2500  -> Accuracy: 0.9806\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09978814918317673\n",
      "Epoch 100, Cost: 0.013602593682582601\n",
      "Epoch 200, Cost: 0.008193250312809\n",
      "Epoch 300, Cost: 0.006583367052211975\n",
      "Epoch 400, Cost: 0.005625057537632896\n",
      "LR: 0.1    Epochs: 500   -> Accuracy: 0.9806\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09980913530613593\n",
      "Epoch 100, Cost: 0.014575053005133051\n",
      "Epoch 200, Cost: 0.008258199294738875\n",
      "Epoch 300, Cost: 0.006643956001257761\n",
      "Epoch 400, Cost: 0.005705965762344672\n",
      "Epoch 500, Cost: 0.005041940388365993\n",
      "Epoch 600, Cost: 0.004529142748585292\n",
      "Epoch 700, Cost: 0.004100261550448069\n",
      "Epoch 800, Cost: 0.0037340754446546923\n",
      "Epoch 900, Cost: 0.0034210924699644403\n",
      "LR: 0.1    Epochs: 1000  -> Accuracy: 0.9778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09972247190554538\n",
      "Epoch 100, Cost: 0.015132335762949119\n",
      "Epoch 200, Cost: 0.008201190524922993\n",
      "Epoch 300, Cost: 0.006558488285964611\n",
      "Epoch 400, Cost: 0.005658382803251744\n",
      "Epoch 500, Cost: 0.005012823991222524\n",
      "Epoch 600, Cost: 0.004512533585222025\n",
      "Epoch 700, Cost: 0.00411268015967206\n",
      "Epoch 800, Cost: 0.0037833251764227468\n",
      "Epoch 900, Cost: 0.003504076801160455\n",
      "Epoch 1000, Cost: 0.003266404633268728\n",
      "Epoch 1100, Cost: 0.0030607733091584863\n",
      "Epoch 1200, Cost: 0.002878874295021001\n",
      "Epoch 1300, Cost: 0.002718131207651502\n",
      "Epoch 1400, Cost: 0.0025767458937030175\n",
      "LR: 0.1    Epochs: 1500  -> Accuracy: 0.9778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09970016241806917\n",
      "Epoch 100, Cost: 0.013831916500948747\n",
      "Epoch 200, Cost: 0.008377313397855872\n",
      "Epoch 300, Cost: 0.006702903738725866\n",
      "Epoch 400, Cost: 0.005687018631406535\n",
      "Epoch 500, Cost: 0.004950166873995108\n",
      "Epoch 600, Cost: 0.0043524478283625875\n",
      "Epoch 700, Cost: 0.003867129868049561\n",
      "Epoch 800, Cost: 0.003493687369350491\n",
      "Epoch 900, Cost: 0.003191086785756687\n",
      "Epoch 1000, Cost: 0.0029411448542429786\n",
      "Epoch 1100, Cost: 0.0027285987937565423\n",
      "Epoch 1200, Cost: 0.002549317814263361\n",
      "Epoch 1300, Cost: 0.0023956179155181336\n",
      "Epoch 1400, Cost: 0.002242759236691746\n",
      "Epoch 1500, Cost: 0.002106012074502562\n",
      "Epoch 1600, Cost: 0.001991063674131135\n",
      "Epoch 1700, Cost: 0.001890819944709187\n",
      "Epoch 1800, Cost: 0.001800223138032966\n",
      "Epoch 1900, Cost: 0.001717332279663629\n",
      "LR: 0.1    Epochs: 2000  -> Accuracy: 0.9806\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09965332845048822\n",
      "Epoch 100, Cost: 0.01532787584817836\n",
      "Epoch 200, Cost: 0.008507932092469594\n",
      "Epoch 300, Cost: 0.006794283678938764\n",
      "Epoch 400, Cost: 0.00576331291762395\n",
      "Epoch 500, Cost: 0.005026098516439989\n",
      "Epoch 600, Cost: 0.004451645928879945\n",
      "Epoch 700, Cost: 0.003996749484809421\n",
      "Epoch 800, Cost: 0.0036284846455736336\n",
      "Epoch 900, Cost: 0.003326611738491962\n",
      "Epoch 1000, Cost: 0.0030725167539754758\n",
      "Epoch 1100, Cost: 0.0028517741167189514\n",
      "Epoch 1200, Cost: 0.0026554703768020124\n",
      "Epoch 1300, Cost: 0.0024880974853926908\n",
      "Epoch 1400, Cost: 0.0023417823982279083\n",
      "Epoch 1500, Cost: 0.002210250143498292\n",
      "Epoch 1600, Cost: 0.002093603313376217\n",
      "Epoch 1700, Cost: 0.0019894585334130884\n",
      "Epoch 1800, Cost: 0.0018942986537283238\n",
      "Epoch 1900, Cost: 0.0018108315462446371\n",
      "Epoch 2000, Cost: 0.0017339863075504522\n",
      "Epoch 2100, Cost: 0.0016648706534568825\n",
      "Epoch 2200, Cost: 0.0016022902849684585\n",
      "Epoch 2300, Cost: 0.0015426567812734386\n",
      "Epoch 2400, Cost: 0.001487784824451008\n",
      "LR: 0.1    Epochs: 2500  -> Accuracy: 0.9861\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Experimenting with tanh activation ===\n",
      "\n",
      "Epoch 0, Cost: 0.10001132327952468\n",
      "Epoch 100, Cost: 0.09625120021128011\n",
      "Epoch 200, Cost: 0.09358865893468675\n",
      "Epoch 300, Cost: 0.09163723993303836\n",
      "Epoch 400, Cost: 0.09013128766123904\n",
      "LR: 0.001  Epochs: 500   -> Accuracy: 0.5333\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10017114534546026\n",
      "Epoch 100, Cost: 0.09640435561795486\n",
      "Epoch 200, Cost: 0.09372471430975253\n",
      "Epoch 300, Cost: 0.09174683575693078\n",
      "Epoch 400, Cost: 0.09020505541687882\n",
      "Epoch 500, Cost: 0.08891453879451562\n",
      "Epoch 600, Cost: 0.0877450718361509\n",
      "Epoch 700, Cost: 0.08660421793551916\n",
      "Epoch 800, Cost: 0.0854272520636897\n",
      "Epoch 900, Cost: 0.08417184722962004\n",
      "LR: 0.001  Epochs: 1000  -> Accuracy: 0.6556\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10000650237989692\n",
      "Epoch 100, Cost: 0.09625820184008334\n",
      "Epoch 200, Cost: 0.09359167866155914\n",
      "Epoch 300, Cost: 0.09162268462494351\n",
      "Epoch 400, Cost: 0.09008678279985134\n",
      "Epoch 500, Cost: 0.08880037086506037\n",
      "Epoch 600, Cost: 0.08763437668978355\n",
      "Epoch 700, Cost: 0.08649725342566852\n",
      "Epoch 800, Cost: 0.08532466180313512\n",
      "Epoch 900, Cost: 0.0840738508779649\n",
      "Epoch 1000, Cost: 0.08272108650742277\n",
      "Epoch 1100, Cost: 0.08126057343930551\n",
      "Epoch 1200, Cost: 0.07970335857518616\n",
      "Epoch 1300, Cost: 0.07807495563525053\n",
      "Epoch 1400, Cost: 0.07641107826037165\n",
      "LR: 0.001  Epochs: 1500  -> Accuracy: 0.6778\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10011431094701977\n",
      "Epoch 100, Cost: 0.09637212141823398\n",
      "Epoch 200, Cost: 0.09372551424111532\n",
      "Epoch 300, Cost: 0.09178850483898092\n",
      "Epoch 400, Cost: 0.09029569933911508\n",
      "Epoch 500, Cost: 0.08906284993073042\n",
      "Epoch 600, Cost: 0.08796014155043153\n",
      "Epoch 700, Cost: 0.08689481995712528\n",
      "Epoch 800, Cost: 0.08580056818805362\n",
      "Epoch 900, Cost: 0.0846316939082\n",
      "Epoch 1000, Cost: 0.08336053806511957\n",
      "Epoch 1100, Cost: 0.08197658265862891\n",
      "Epoch 1200, Cost: 0.08048571438254874\n",
      "Epoch 1300, Cost: 0.07890830252130046\n",
      "Epoch 1400, Cost: 0.0772754134806571\n",
      "Epoch 1500, Cost: 0.07562349924337204\n",
      "Epoch 1600, Cost: 0.07398874395784519\n",
      "Epoch 1700, Cost: 0.07240240483568654\n",
      "Epoch 1800, Cost: 0.07088792821248775\n",
      "Epoch 1900, Cost: 0.06945987145769235\n",
      "LR: 0.001  Epochs: 2000  -> Accuracy: 0.7806\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09991686928345096\n",
      "Epoch 100, Cost: 0.09616066159254948\n",
      "Epoch 200, Cost: 0.09349054249976663\n",
      "Epoch 300, Cost: 0.09152232313178706\n",
      "Epoch 400, Cost: 0.08999143212807818\n",
      "Epoch 500, Cost: 0.08871377432221222\n",
      "Epoch 600, Cost: 0.08755916311724357\n",
      "Epoch 700, Cost: 0.08643399733192485\n",
      "Epoch 800, Cost: 0.08527065275325031\n",
      "Epoch 900, Cost: 0.08402174069516793\n",
      "Epoch 1000, Cost: 0.08265776781431176\n",
      "Epoch 1100, Cost: 0.08116679670258437\n",
      "Epoch 1200, Cost: 0.0795545822367902\n",
      "Epoch 1300, Cost: 0.07784362459598969\n",
      "Epoch 1400, Cost: 0.07606997512803877\n",
      "Epoch 1500, Cost: 0.0742775937100548\n",
      "Epoch 1600, Cost: 0.0725112716120451\n",
      "Epoch 1700, Cost: 0.07080993485145055\n",
      "Epoch 1800, Cost: 0.06920202819167556\n",
      "Epoch 1900, Cost: 0.06770377141693316\n",
      "Epoch 2000, Cost: 0.06632000211940302\n",
      "Epoch 2100, Cost: 0.06504665144462161\n",
      "Epoch 2200, Cost: 0.06387379862840731\n",
      "Epoch 2300, Cost: 0.06278852274431622\n",
      "Epoch 2400, Cost: 0.06177714339704299\n",
      "LR: 0.001  Epochs: 2500  -> Accuracy: 0.8444\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.099971940141545\n",
      "Epoch 100, Cost: 0.08866218346223009\n",
      "Epoch 200, Cost: 0.08248621214928019\n",
      "Epoch 300, Cost: 0.07418648157489144\n",
      "Epoch 400, Cost: 0.06660985325931867\n",
      "LR: 0.005  Epochs: 500   -> Accuracy: 0.8056\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.099941917228441\n",
      "Epoch 100, Cost: 0.08877515278190476\n",
      "Epoch 200, Cost: 0.08280300752745902\n",
      "Epoch 300, Cost: 0.07499341067763202\n",
      "Epoch 400, Cost: 0.06773008488051958\n",
      "Epoch 500, Cost: 0.06243136094231894\n",
      "Epoch 600, Cost: 0.05819329252484141\n",
      "Epoch 700, Cost: 0.0543190537025072\n",
      "Epoch 800, Cost: 0.050786292547695996\n",
      "Epoch 900, Cost: 0.04780590616109475\n",
      "LR: 0.005  Epochs: 1000  -> Accuracy: 0.9194\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10005575981882533\n",
      "Epoch 100, Cost: 0.08849766864259798\n",
      "Epoch 200, Cost: 0.08194358951986341\n",
      "Epoch 300, Cost: 0.07409135431591796\n",
      "Epoch 400, Cost: 0.06727778737359667\n",
      "Epoch 500, Cost: 0.0623269570984248\n",
      "Epoch 600, Cost: 0.058277463242740024\n",
      "Epoch 700, Cost: 0.054610645265695264\n",
      "Epoch 800, Cost: 0.05137590674167336\n",
      "Epoch 900, Cost: 0.0486574505981701\n",
      "Epoch 1000, Cost: 0.04642851827506723\n",
      "Epoch 1100, Cost: 0.044613725266719834\n",
      "Epoch 1200, Cost: 0.04314044507239163\n",
      "Epoch 1300, Cost: 0.04194385837501748\n",
      "Epoch 1400, Cost: 0.04096615013728399\n",
      "LR: 0.005  Epochs: 1500  -> Accuracy: 0.9194\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09980811206573632\n",
      "Epoch 100, Cost: 0.08838453362598243\n",
      "Epoch 200, Cost: 0.08189822345632308\n",
      "Epoch 300, Cost: 0.07378109054806521\n",
      "Epoch 400, Cost: 0.06660780734588162\n",
      "Epoch 500, Cost: 0.06125763314601031\n",
      "Epoch 600, Cost: 0.05682741680670472\n",
      "Epoch 700, Cost: 0.052987395308331076\n",
      "Epoch 800, Cost: 0.049835453262841986\n",
      "Epoch 900, Cost: 0.04739921599708361\n",
      "Epoch 1000, Cost: 0.04555585022421113\n",
      "Epoch 1100, Cost: 0.04413696306986516\n",
      "Epoch 1200, Cost: 0.04299972478644333\n",
      "Epoch 1300, Cost: 0.04204363337788769\n",
      "Epoch 1400, Cost: 0.04120576665680285\n",
      "Epoch 1500, Cost: 0.040452039844093034\n",
      "Epoch 1600, Cost: 0.039768371229415046\n",
      "Epoch 1700, Cost: 0.039151812943968904\n",
      "Epoch 1800, Cost: 0.03860270072454533\n",
      "Epoch 1900, Cost: 0.03811960942689195\n",
      "LR: 0.005  Epochs: 2000  -> Accuracy: 0.9306\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09998669683011781\n",
      "Epoch 100, Cost: 0.0889119245293271\n",
      "Epoch 200, Cost: 0.08319183213708893\n",
      "Epoch 300, Cost: 0.07617589626120495\n",
      "Epoch 400, Cost: 0.06930351212526173\n",
      "Epoch 500, Cost: 0.06386551803586998\n",
      "Epoch 600, Cost: 0.05961423953566477\n",
      "Epoch 700, Cost: 0.055811050883010006\n",
      "Epoch 800, Cost: 0.05228632142739011\n",
      "Epoch 900, Cost: 0.04920672913487728\n",
      "Epoch 1000, Cost: 0.04670361726955526\n",
      "Epoch 1100, Cost: 0.04474595539183587\n",
      "Epoch 1200, Cost: 0.04321076781652868\n",
      "Epoch 1300, Cost: 0.041977874312214315\n",
      "Epoch 1400, Cost: 0.0409648412906918\n",
      "Epoch 1500, Cost: 0.0401213927005336\n",
      "Epoch 1600, Cost: 0.03941533253855205\n",
      "Epoch 1700, Cost: 0.038822652734091856\n",
      "Epoch 1800, Cost: 0.03832299157027298\n",
      "Epoch 1900, Cost: 0.037898448368472136\n",
      "Epoch 2000, Cost: 0.0375337044878495\n",
      "Epoch 2100, Cost: 0.03721622708444878\n",
      "Epoch 2200, Cost: 0.036936153005808055\n",
      "Epoch 2300, Cost: 0.03668590410067556\n",
      "Epoch 2400, Cost: 0.0364597034959609\n",
      "LR: 0.005  Epochs: 2500  -> Accuracy: 0.9444\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10012065494691653\n",
      "Epoch 100, Cost: 0.0824821646503803\n",
      "Epoch 200, Cost: 0.0679657360966417\n",
      "Epoch 300, Cost: 0.05844193625814944\n",
      "Epoch 400, Cost: 0.05127819253172469\n",
      "LR: 0.01   Epochs: 500   -> Accuracy: 0.9000\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10007145685739191\n",
      "Epoch 100, Cost: 0.08316078072538058\n",
      "Epoch 200, Cost: 0.06695377773498197\n",
      "Epoch 300, Cost: 0.05672618235464325\n",
      "Epoch 400, Cost: 0.049987933245590345\n",
      "Epoch 500, Cost: 0.04537825782511303\n",
      "Epoch 600, Cost: 0.04238306857134689\n",
      "Epoch 700, Cost: 0.04036415647966658\n",
      "Epoch 800, Cost: 0.03896269369416895\n",
      "Epoch 900, Cost: 0.037979851666034244\n",
      "LR: 0.01   Epochs: 1000  -> Accuracy: 0.9444\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09991866093404975\n",
      "Epoch 100, Cost: 0.08224917631216488\n",
      "Epoch 200, Cost: 0.06773725549025598\n",
      "Epoch 300, Cost: 0.05836512395439148\n",
      "Epoch 400, Cost: 0.05120953868854908\n",
      "Epoch 500, Cost: 0.04594065981838489\n",
      "Epoch 600, Cost: 0.04261800436476242\n",
      "Epoch 700, Cost: 0.04051633760087792\n",
      "Epoch 800, Cost: 0.03910289450715684\n",
      "Epoch 900, Cost: 0.03811318600756804\n",
      "Epoch 1000, Cost: 0.037392454707578425\n",
      "Epoch 1100, Cost: 0.03684141485707918\n",
      "Epoch 1200, Cost: 0.036400444615719514\n",
      "Epoch 1300, Cost: 0.036034452794708995\n",
      "Epoch 1400, Cost: 0.03572172317517184\n",
      "LR: 0.01   Epochs: 1500  -> Accuracy: 0.9444\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09988536012077244\n",
      "Epoch 100, Cost: 0.08236412720537081\n",
      "Epoch 200, Cost: 0.06722029180791685\n",
      "Epoch 300, Cost: 0.058665633786779976\n",
      "Epoch 400, Cost: 0.051912452872449937\n",
      "Epoch 500, Cost: 0.04665411003193593\n",
      "Epoch 600, Cost: 0.04299112371743935\n",
      "Epoch 700, Cost: 0.04060592448025033\n",
      "Epoch 800, Cost: 0.039076327994468335\n",
      "Epoch 900, Cost: 0.038052714764881826\n",
      "Epoch 1000, Cost: 0.03731980776510765\n",
      "Epoch 1100, Cost: 0.03675619142978195\n",
      "Epoch 1200, Cost: 0.036296004896173774\n",
      "Epoch 1300, Cost: 0.035902865906825815\n",
      "Epoch 1400, Cost: 0.0355551572741328\n",
      "Epoch 1500, Cost: 0.035238825870176564\n",
      "Epoch 1600, Cost: 0.034944018425440884\n",
      "Epoch 1700, Cost: 0.034663478806888234\n",
      "Epoch 1800, Cost: 0.03439176768495433\n",
      "Epoch 1900, Cost: 0.034124888889126286\n",
      "LR: 0.01   Epochs: 2000  -> Accuracy: 0.9500\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09977659138746969\n",
      "Epoch 100, Cost: 0.08167590673555197\n",
      "Epoch 200, Cost: 0.06717754636534615\n",
      "Epoch 300, Cost: 0.05878584665028285\n",
      "Epoch 400, Cost: 0.05187705680677652\n",
      "Epoch 500, Cost: 0.04672789954042784\n",
      "Epoch 600, Cost: 0.043193506414518475\n",
      "Epoch 700, Cost: 0.040862075347858386\n",
      "Epoch 800, Cost: 0.03930181922620019\n",
      "Epoch 900, Cost: 0.038221493634203366\n",
      "Epoch 1000, Cost: 0.03744444302768413\n",
      "Epoch 1100, Cost: 0.03685609573796401\n",
      "Epoch 1200, Cost: 0.03638712343295449\n",
      "Epoch 1300, Cost: 0.03599750388880273\n",
      "Epoch 1400, Cost: 0.03566324310623074\n",
      "Epoch 1500, Cost: 0.03536885594043095\n",
      "Epoch 1600, Cost: 0.035103687645311535\n",
      "Epoch 1700, Cost: 0.034860043845882926\n",
      "Epoch 1800, Cost: 0.03463212278946843\n",
      "Epoch 1900, Cost: 0.034415352987623375\n",
      "Epoch 2000, Cost: 0.03420597287681251\n",
      "Epoch 2100, Cost: 0.03400076995377353\n",
      "Epoch 2200, Cost: 0.033796928952211605\n",
      "Epoch 2300, Cost: 0.03359195472849828\n",
      "Epoch 2400, Cost: 0.03338364588638637\n",
      "LR: 0.01   Epochs: 2500  -> Accuracy: 0.9556\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10022466980137362\n",
      "Epoch 100, Cost: 0.04728382945129609\n",
      "Epoch 200, Cost: 0.03750157738494054\n",
      "Epoch 300, Cost: 0.035363733424187985\n",
      "Epoch 400, Cost: 0.033913984821687365\n",
      "LR: 0.05   Epochs: 500   -> Accuracy: 0.9556\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10009441698095835\n",
      "Epoch 100, Cost: 0.046598777512874015\n",
      "Epoch 200, Cost: 0.03750663415146573\n",
      "Epoch 300, Cost: 0.03536977077833742\n",
      "Epoch 400, Cost: 0.0341161932065669\n",
      "Epoch 500, Cost: 0.03293614076108925\n",
      "Epoch 600, Cost: 0.03167361375963852\n",
      "Epoch 700, Cost: 0.030350425796673332\n",
      "Epoch 800, Cost: 0.029005757400559936\n",
      "Epoch 900, Cost: 0.027670254547020386\n",
      "LR: 0.05   Epochs: 1000  -> Accuracy: 0.9556\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10001395699312776\n",
      "Epoch 100, Cost: 0.046022578223330154\n",
      "Epoch 200, Cost: 0.03741457249980003\n",
      "Epoch 300, Cost: 0.03546841023781766\n",
      "Epoch 400, Cost: 0.03434403624763197\n",
      "Epoch 500, Cost: 0.03327969479324947\n",
      "Epoch 600, Cost: 0.03203791597773952\n",
      "Epoch 700, Cost: 0.03058699116891265\n",
      "Epoch 800, Cost: 0.0290216563717676\n",
      "Epoch 900, Cost: 0.02746037040510689\n",
      "Epoch 1000, Cost: 0.02598558880166789\n",
      "Epoch 1100, Cost: 0.024649789760395165\n",
      "Epoch 1200, Cost: 0.023480097466547237\n",
      "Epoch 1300, Cost: 0.022470459074913695\n",
      "Epoch 1400, Cost: 0.021595218465752183\n",
      "LR: 0.05   Epochs: 1500  -> Accuracy: 0.9639\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09990571295643949\n",
      "Epoch 100, Cost: 0.04646981378650126\n",
      "Epoch 200, Cost: 0.03734495005602565\n",
      "Epoch 300, Cost: 0.035238055927523\n",
      "Epoch 400, Cost: 0.033939787519088604\n",
      "Epoch 500, Cost: 0.032681067928951674\n",
      "Epoch 600, Cost: 0.03130810143732385\n",
      "Epoch 700, Cost: 0.02981748034271447\n",
      "Epoch 800, Cost: 0.02829944476517967\n",
      "Epoch 900, Cost: 0.026870608888869423\n",
      "Epoch 1000, Cost: 0.025585333229648177\n",
      "Epoch 1100, Cost: 0.024436917445443646\n",
      "Epoch 1200, Cost: 0.023400272796718562\n",
      "Epoch 1300, Cost: 0.022453787193873198\n",
      "Epoch 1400, Cost: 0.02158054073077345\n",
      "Epoch 1500, Cost: 0.02076981102302601\n",
      "Epoch 1600, Cost: 0.020019062202608722\n",
      "Epoch 1700, Cost: 0.01933060704898301\n",
      "Epoch 1800, Cost: 0.01870603817046457\n",
      "Epoch 1900, Cost: 0.018143857646995207\n",
      "LR: 0.05   Epochs: 2000  -> Accuracy: 0.9667\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09998382823670483\n",
      "Epoch 100, Cost: 0.046429932566290515\n",
      "Epoch 200, Cost: 0.03751414863414345\n",
      "Epoch 300, Cost: 0.03540918377069711\n",
      "Epoch 400, Cost: 0.0342609353232684\n",
      "Epoch 500, Cost: 0.03323917345122461\n",
      "Epoch 600, Cost: 0.03207410222669605\n",
      "Epoch 700, Cost: 0.03069151901875312\n",
      "Epoch 800, Cost: 0.029180210369070567\n",
      "Epoch 900, Cost: 0.027678273921407717\n",
      "Epoch 1000, Cost: 0.026275073832828524\n",
      "Epoch 1100, Cost: 0.02500492567723814\n",
      "Epoch 1200, Cost: 0.02386603770342709\n",
      "Epoch 1300, Cost: 0.02284551258002686\n",
      "Epoch 1400, Cost: 0.021932951443272616\n",
      "Epoch 1500, Cost: 0.02112122554602547\n",
      "Epoch 1600, Cost: 0.020402074052440933\n",
      "Epoch 1700, Cost: 0.019763422978736653\n",
      "Epoch 1800, Cost: 0.019190713444283976\n",
      "Epoch 1900, Cost: 0.01866990899016125\n",
      "Epoch 2000, Cost: 0.01818951196610367\n",
      "Epoch 2100, Cost: 0.017740577024834624\n",
      "Epoch 2200, Cost: 0.01731575617133734\n",
      "Epoch 2300, Cost: 0.016908957170174512\n",
      "Epoch 2400, Cost: 0.016515713133169498\n",
      "LR: 0.05   Epochs: 2500  -> Accuracy: 0.9639\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10013798017115882\n",
      "Epoch 100, Cost: 0.0376375109932201\n",
      "Epoch 200, Cost: 0.03449039911015056\n",
      "Epoch 300, Cost: 0.032542499955112644\n",
      "Epoch 400, Cost: 0.029873611303126\n",
      "LR: 0.1    Epochs: 500   -> Accuracy: 0.9556\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10007844695392722\n",
      "Epoch 100, Cost: 0.03733664710505226\n",
      "Epoch 200, Cost: 0.03390273344286132\n",
      "Epoch 300, Cost: 0.030733770337833023\n",
      "Epoch 400, Cost: 0.02727441124502991\n",
      "Epoch 500, Cost: 0.02478332215733218\n",
      "Epoch 600, Cost: 0.022927750868913813\n",
      "Epoch 700, Cost: 0.021387620408083376\n",
      "Epoch 800, Cost: 0.020060019659985336\n",
      "Epoch 900, Cost: 0.01891772681119949\n",
      "LR: 0.1    Epochs: 1000  -> Accuracy: 0.9583\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.09991482432335455\n",
      "Epoch 100, Cost: 0.03719015144602102\n",
      "Epoch 200, Cost: 0.033744363533865555\n",
      "Epoch 300, Cost: 0.031038362458356734\n",
      "Epoch 400, Cost: 0.02842144238748023\n",
      "Epoch 500, Cost: 0.02576583665578255\n",
      "Epoch 600, Cost: 0.02338437927878959\n",
      "Epoch 700, Cost: 0.021475899271246487\n",
      "Epoch 800, Cost: 0.01997831591095372\n",
      "Epoch 900, Cost: 0.018785283608146647\n",
      "Epoch 1000, Cost: 0.017810387091985576\n",
      "Epoch 1100, Cost: 0.016986670572672058\n",
      "Epoch 1200, Cost: 0.01626277886468268\n",
      "Epoch 1300, Cost: 0.015603296765693091\n",
      "Epoch 1400, Cost: 0.014990486863948884\n",
      "LR: 0.1    Epochs: 1500  -> Accuracy: 0.9611\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10005352295808279\n",
      "Epoch 100, Cost: 0.03753892779532131\n",
      "Epoch 200, Cost: 0.03419463892207067\n",
      "Epoch 300, Cost: 0.03194944815578339\n",
      "Epoch 400, Cost: 0.029178202076122338\n",
      "Epoch 500, Cost: 0.026183742237892153\n",
      "Epoch 600, Cost: 0.023453487648774078\n",
      "Epoch 700, Cost: 0.02126263425825594\n",
      "Epoch 800, Cost: 0.01948222377455871\n",
      "Epoch 900, Cost: 0.018002641931970265\n",
      "Epoch 1000, Cost: 0.01679855319338412\n",
      "Epoch 1100, Cost: 0.015836147645604527\n",
      "Epoch 1200, Cost: 0.015049775977986875\n",
      "Epoch 1300, Cost: 0.014387410438848915\n",
      "Epoch 1400, Cost: 0.013817400562830191\n",
      "Epoch 1500, Cost: 0.013322433528010841\n",
      "Epoch 1600, Cost: 0.01289188911219679\n",
      "Epoch 1700, Cost: 0.012515138522592277\n",
      "Epoch 1800, Cost: 0.012181579249680203\n",
      "Epoch 1900, Cost: 0.011882465944974592\n",
      "LR: 0.1    Epochs: 2000  -> Accuracy: 0.9722\n",
      "\n",
      "\n",
      "\n",
      "Epoch 0, Cost: 0.10019860331591197\n",
      "Epoch 100, Cost: 0.03756206530101346\n",
      "Epoch 200, Cost: 0.033852852790307475\n",
      "Epoch 300, Cost: 0.03117946971794698\n",
      "Epoch 400, Cost: 0.028672345843413636\n",
      "Epoch 500, Cost: 0.026350295609028743\n",
      "Epoch 600, Cost: 0.024364298380866758\n",
      "Epoch 700, Cost: 0.02267593273933562\n",
      "Epoch 800, Cost: 0.02114227785835855\n",
      "Epoch 900, Cost: 0.019714951290887184\n",
      "Epoch 1000, Cost: 0.018408754420619237\n",
      "Epoch 1100, Cost: 0.017258890713430085\n",
      "Epoch 1200, Cost: 0.016292330525744244\n",
      "Epoch 1300, Cost: 0.015510239380735648\n",
      "Epoch 1400, Cost: 0.014884798533347041\n",
      "Epoch 1500, Cost: 0.0143774062988369\n",
      "Epoch 1600, Cost: 0.013954501751823303\n",
      "Epoch 1700, Cost: 0.013591065817729701\n",
      "Epoch 1800, Cost: 0.013270394059292178\n",
      "Epoch 1900, Cost: 0.012982286864053194\n",
      "Epoch 2000, Cost: 0.012720648880267993\n",
      "Epoch 2100, Cost: 0.01248145997312423\n",
      "Epoch 2200, Cost: 0.012261506983430799\n",
      "Epoch 2300, Cost: 0.012057943491774838\n",
      "Epoch 2400, Cost: 0.011868292056045174\n",
      "LR: 0.1    Epochs: 2500  -> Accuracy: 0.9667\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grids for learning rate and number of epochs to experiment with\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "epoch_vals = [500, 1000, 1500, 2000, 2500]\n",
    "activation_names = [\"sigmoid\", \"ReLU\", \"tanh\"]\n",
    "results = {}\n",
    "\n",
    "# Loop over each activation function\n",
    "for act_name in activation_names:\n",
    "    results[act_name] = {}  # Create an entry in results for the current activation\n",
    "    print(f\"\\n=== Experimenting with {act_name} activation ===\\n\")\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for ep in epoch_vals:\n",
    "            net = NeuralNetwork(learning_rate=lr, epochs=ep)\n",
    "\n",
    "            # Retrieve the activation function and its derivative based on the current activation name\n",
    "            activation = getattr(net, act_name)\n",
    "            activation_deriv = getattr(net, act_name + \"_deriv\")\n",
    "            \n",
    "            # Train the network using the chosen activation functions\n",
    "            net.train(X_train, y_train, activation=activation, activation_derivative=activation_deriv)\n",
    "            \n",
    "            predictions = net.predict(X_test, activation=activation)\n",
    "            y_test_labels = np.argmax(y_test, axis=1) \n",
    "            acc = net.accuracy_score(y_test_labels, predictions)\n",
    "\n",
    "            # Store the accuracy in the results dictionary using (learning rate, epochs) tuple as the key\n",
    "            results[act_name][(lr, ep)] = acc\n",
    "            \n",
    "            print(f\"LR: {lr:<6} Epochs: {ep:<5} -> Accuracy: {acc:.4f}\")\n",
    "            print(\"\\n\\n\")\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c17fc5e7-01d0-4da4-8490-cc7f5ef7cd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***** Summary of All Hyperparameter Experiments *****\n",
      "\n",
      "Activation Function: sigmoid\n",
      "----------------------------------------\n",
      "Learning Rate: 0.001  | Epochs: 500   | Accuracy: 0.0833\n",
      "Learning Rate: 0.001  | Epochs: 1000  | Accuracy: 0.0944\n",
      "Learning Rate: 0.001  | Epochs: 1500  | Accuracy: 0.0806\n",
      "Learning Rate: 0.001  | Epochs: 2000  | Accuracy: 0.0778\n",
      "Learning Rate: 0.001  | Epochs: 2500  | Accuracy: 0.0778\n",
      "Learning Rate: 0.005  | Epochs: 500   | Accuracy: 0.0778\n",
      "Learning Rate: 0.005  | Epochs: 1000  | Accuracy: 0.0778\n",
      "Learning Rate: 0.005  | Epochs: 1500  | Accuracy: 0.0778\n",
      "Learning Rate: 0.005  | Epochs: 2000  | Accuracy: 0.0778\n",
      "Learning Rate: 0.005  | Epochs: 2500  | Accuracy: 0.0778\n",
      "Learning Rate: 0.01   | Epochs: 500   | Accuracy: 0.0778\n",
      "Learning Rate: 0.01   | Epochs: 1000  | Accuracy: 0.0778\n",
      "Learning Rate: 0.01   | Epochs: 1500  | Accuracy: 0.0778\n",
      "Learning Rate: 0.01   | Epochs: 2000  | Accuracy: 0.0778\n",
      "Learning Rate: 0.01   | Epochs: 2500  | Accuracy: 0.0778\n",
      "Learning Rate: 0.05   | Epochs: 500   | Accuracy: 0.0778\n",
      "Learning Rate: 0.05   | Epochs: 1000  | Accuracy: 0.1111\n",
      "Learning Rate: 0.05   | Epochs: 1500  | Accuracy: 0.5361\n",
      "Learning Rate: 0.05   | Epochs: 2000  | Accuracy: 0.5417\n",
      "Learning Rate: 0.05   | Epochs: 2500  | Accuracy: 0.6083\n",
      "Learning Rate: 0.1    | Epochs: 500   | Accuracy: 0.0889\n",
      "Learning Rate: 0.1    | Epochs: 1000  | Accuracy: 0.5222\n",
      "Learning Rate: 0.1    | Epochs: 1500  | Accuracy: 0.7111\n",
      "Learning Rate: 0.1    | Epochs: 2000  | Accuracy: 0.8361\n",
      "Learning Rate: 0.1    | Epochs: 2500  | Accuracy: 0.8806\n",
      "----------------------------------------\n",
      "\n",
      "Activation Function: ReLU\n",
      "----------------------------------------\n",
      "Learning Rate: 0.001  | Epochs: 500   | Accuracy: 0.3167\n",
      "Learning Rate: 0.001  | Epochs: 1000  | Accuracy: 0.3444\n",
      "Learning Rate: 0.001  | Epochs: 1500  | Accuracy: 0.6417\n",
      "Learning Rate: 0.001  | Epochs: 2000  | Accuracy: 0.6389\n",
      "Learning Rate: 0.001  | Epochs: 2500  | Accuracy: 0.6667\n",
      "Learning Rate: 0.005  | Epochs: 500   | Accuracy: 0.6472\n",
      "Learning Rate: 0.005  | Epochs: 1000  | Accuracy: 0.8333\n",
      "Learning Rate: 0.005  | Epochs: 1500  | Accuracy: 0.9139\n",
      "Learning Rate: 0.005  | Epochs: 2000  | Accuracy: 0.9389\n",
      "Learning Rate: 0.005  | Epochs: 2500  | Accuracy: 0.9472\n",
      "Learning Rate: 0.01   | Epochs: 500   | Accuracy: 0.8250\n",
      "Learning Rate: 0.01   | Epochs: 1000  | Accuracy: 0.9361\n",
      "Learning Rate: 0.01   | Epochs: 1500  | Accuracy: 0.9500\n",
      "Learning Rate: 0.01   | Epochs: 2000  | Accuracy: 0.9639\n",
      "Learning Rate: 0.01   | Epochs: 2500  | Accuracy: 0.9778\n",
      "Learning Rate: 0.05   | Epochs: 500   | Accuracy: 0.9778\n",
      "Learning Rate: 0.05   | Epochs: 1000  | Accuracy: 0.9806\n",
      "Learning Rate: 0.05   | Epochs: 1500  | Accuracy: 0.9833\n",
      "Learning Rate: 0.05   | Epochs: 2000  | Accuracy: 0.9806\n",
      "Learning Rate: 0.05   | Epochs: 2500  | Accuracy: 0.9806\n",
      "Learning Rate: 0.1    | Epochs: 500   | Accuracy: 0.9806\n",
      "Learning Rate: 0.1    | Epochs: 1000  | Accuracy: 0.9778\n",
      "Learning Rate: 0.1    | Epochs: 1500  | Accuracy: 0.9778\n",
      "Learning Rate: 0.1    | Epochs: 2000  | Accuracy: 0.9806\n",
      "Learning Rate: 0.1    | Epochs: 2500  | Accuracy: 0.9861\n",
      "----------------------------------------\n",
      "\n",
      "Activation Function: tanh\n",
      "----------------------------------------\n",
      "Learning Rate: 0.001  | Epochs: 500   | Accuracy: 0.5333\n",
      "Learning Rate: 0.001  | Epochs: 1000  | Accuracy: 0.6556\n",
      "Learning Rate: 0.001  | Epochs: 1500  | Accuracy: 0.6778\n",
      "Learning Rate: 0.001  | Epochs: 2000  | Accuracy: 0.7806\n",
      "Learning Rate: 0.001  | Epochs: 2500  | Accuracy: 0.8444\n",
      "Learning Rate: 0.005  | Epochs: 500   | Accuracy: 0.8056\n",
      "Learning Rate: 0.005  | Epochs: 1000  | Accuracy: 0.9194\n",
      "Learning Rate: 0.005  | Epochs: 1500  | Accuracy: 0.9194\n",
      "Learning Rate: 0.005  | Epochs: 2000  | Accuracy: 0.9306\n",
      "Learning Rate: 0.005  | Epochs: 2500  | Accuracy: 0.9444\n",
      "Learning Rate: 0.01   | Epochs: 500   | Accuracy: 0.9000\n",
      "Learning Rate: 0.01   | Epochs: 1000  | Accuracy: 0.9444\n",
      "Learning Rate: 0.01   | Epochs: 1500  | Accuracy: 0.9444\n",
      "Learning Rate: 0.01   | Epochs: 2000  | Accuracy: 0.9500\n",
      "Learning Rate: 0.01   | Epochs: 2500  | Accuracy: 0.9556\n",
      "Learning Rate: 0.05   | Epochs: 500   | Accuracy: 0.9556\n",
      "Learning Rate: 0.05   | Epochs: 1000  | Accuracy: 0.9556\n",
      "Learning Rate: 0.05   | Epochs: 1500  | Accuracy: 0.9639\n",
      "Learning Rate: 0.05   | Epochs: 2000  | Accuracy: 0.9667\n",
      "Learning Rate: 0.05   | Epochs: 2500  | Accuracy: 0.9639\n",
      "Learning Rate: 0.1    | Epochs: 500   | Accuracy: 0.9556\n",
      "Learning Rate: 0.1    | Epochs: 1000  | Accuracy: 0.9583\n",
      "Learning Rate: 0.1    | Epochs: 1500  | Accuracy: 0.9611\n",
      "Learning Rate: 0.1    | Epochs: 2000  | Accuracy: 0.9722\n",
      "Learning Rate: 0.1    | Epochs: 2500  | Accuracy: 0.9667\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n***** Summary of All Hyperparameter Experiments *****\\n\")\n",
    "\n",
    "for act_name, res in results.items():\n",
    "    print(f\"Activation Function: {act_name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for (lr, ep), acc in sorted(res.items()):\n",
    "        print(f\"Learning Rate: {lr:<6} | Epochs: {ep:<5} | Accuracy: {acc:.4f}\")\n",
    "    print(\"-\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6a852-1c9a-40ff-8d47-adbded5035b9",
   "metadata": {},
   "source": [
    "## Analysis and Findings\n",
    "\n",
    "**Activation Function Impact:**\n",
    "\n",
    "- **Sigmoid:**  \n",
    "  When using very low learning rates (0.001, 0.005, 0.01), sigmoid activation produced test accuracies in the range of roughly 7.8%–9.4%. However, when the learning rate was increased (to 0.05 and 0.1) and training was extended over more epochs, the network’s performance improved significantly, reaching up to 88.06% accuracy (for example, with a learning rate of 0.1 and 2500 epochs). This indicates that while sigmoid activation can improve with higher learning rates and longer training, it remains highly sensitive to hyperparameter settings and does not reach the performance levels seen with other activations.\n",
    "\n",
    "- **ReLU:**  \n",
    "  ReLU activation demonstrated strong performance. For very low learning rates, the accuracy was relatively low (around 31–34% with 500–1000 epochs). With moderate learning rates (such as 0.005 and 0.01), the accuracy improved steadily—reaching approximately 97.78% at a learning rate of 0.01 and 2500 epochs. At higher learning rates (0.05 and 0.1), the performance increased further, peaking at around 98.61% (with a learning rate of 0.1 at 2500 epochs). This suggests that ReLU is robust for this network architecture when appropriately tuned.\n",
    "\n",
    "- **Tanh:**  \n",
    "  Tanh activation showed a gradual and stable improvement as hyperparameters were increased. With a very low learning rate, accuracies started around 53.33% and gradually improved to about 84.44% at 2500 epochs for a learning rate of 0.001. With moderate to high learning rates (0.005 and 0.01), the accuracies improved further (up to 95.56% at 2500 epochs). Overall, tanh performed stably with final accuracies in the mid-to-high 90%s.\n",
    "\n",
    "**Hyperparameter Sensitivity:**\n",
    "\n",
    "- **Learning Rate:**  \n",
    "  Lower learning rates (e.g., 0.001, 0.005, 0.01) resulted in very low accuracies for sigmoid, while moderate to high learning rates (0.05 and 0.1) produced significant improvements. Both ReLU and tanh also benefited from moderate and high learning rates, achieving much higher accuracy levels under these conditions.\n",
    "\n",
    "- **Epochs:**  \n",
    "  Increasing the number of training epochs generally improved accuracy for ReLU and tanh until a plateau was reached. For example, ReLU accuracy improved from around 82.50% at 500 epochs to approximately 98.61% at 2500 epochs. Tanh followed a similar trend, gradually increasing accuracy until optimum training duration.\n",
    "\n",
    "**Overall Comparison:**\n",
    "\n",
    "- **ReLU** achieved the highest peak performance, reaching up to about 98.61% accuracy with optimal hyperparameters.\n",
    "- **Tanh** demonstrated very stable performance, with test accuracies consistently in the high 90%s (up to around 95.56%–97.22%).\n",
    "- **Sigmoid** shows potential improvement at higher learning rates and more epochs (up to 88.06%), but it remains less effective compared to ReLU and tanh.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75750c0-d0c6-4b65-a377-0b28ac329778",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The neural network was built with a fixed architecture of [64, 30, 10] and implemented using forward and backpropagation with a mean squared error cost function. For classifying the MNIST dataset using a neural network with a [64, 30, 10] structure, **ReLU** is the preferred activation function (considering the hyperparameter combinations tested in this notebook) due to its superior peak performance, with **Tanh** as a very strong alternative. While **Sigmoid** can improve with careful tuning, it is less reliable compared to ReLU and tanh because of its sensitivity to the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01d0c9-2d7e-447b-a85c-588bd272eff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
